// Generated by the gRPC C++ plugin.
// If you make any local change, they will be lost.
// source: grpc_service.proto
// Original file comments:
// Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions
// are met:
//  * Redistributions of source code must retain the above copyright
//    notice, this list of conditions and the following disclaimer.
//  * Redistributions in binary form must reproduce the above copyright
//    notice, this list of conditions and the following disclaimer in the
//    documentation and/or other materials provided with the distribution.
//  * Neither the name of NVIDIA CORPORATION nor the names of its
//    contributors may be used to endorse or promote products derived
//    from this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
// OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
//
#ifndef GRPC_grpc_5fservice_2eproto__INCLUDED
#define GRPC_grpc_5fservice_2eproto__INCLUDED

#include "grpc_service.pb.h"

#include <functional>
#include <grpcpp/impl/codegen/async_generic_service.h>
#include <grpcpp/impl/codegen/async_stream.h>
#include <grpcpp/impl/codegen/async_unary_call.h>
#include <grpcpp/impl/codegen/client_callback.h>
#include <grpcpp/impl/codegen/client_context.h>
#include <grpcpp/impl/codegen/completion_queue.h>
#include <grpcpp/impl/codegen/message_allocator.h>
#include <grpcpp/impl/codegen/method_handler.h>
#include <grpcpp/impl/codegen/proto_utils.h>
#include <grpcpp/impl/codegen/rpc_method.h>
#include <grpcpp/impl/codegen/server_callback.h>
#include <grpcpp/impl/codegen/server_callback_handlers.h>
#include <grpcpp/impl/codegen/server_context.h>
#include <grpcpp/impl/codegen/service_type.h>
#include <grpcpp/impl/codegen/status.h>
#include <grpcpp/impl/codegen/stub_options.h>
#include <grpcpp/impl/codegen/sync_stream.h>

namespace nvidia {
namespace inferenceserver {

// @@
// @@.. cpp:var:: service GRPCService
// @@
// @@   Inference Server GRPC endpoints.
// @@
class GRPCService final {
 public:
  static constexpr char const* service_full_name() {
    return "nvidia.inferenceserver.GRPCService";
  }
  class StubInterface {
   public:
    virtual ~StubInterface() {}
    // @@  .. cpp:var:: rpc Status(StatusRequest) returns (StatusResponse)
    // @@
    // @@     Get status for entire inference server or for a specified model.
    // @@
    virtual ::grpc::Status Status(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::nvidia::inferenceserver::StatusResponse* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::StatusResponse>> AsyncStatus(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::StatusResponse>>(AsyncStatusRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::StatusResponse>> PrepareAsyncStatus(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::StatusResponse>>(PrepareAsyncStatusRaw(context, request, cq));
    }
    // @@  .. cpp:var:: rpc Profile(ProfileRequest) returns (ProfileResponse)
    // @@
    // @@     Enable and disable low-level GPU profiling.
    // @@
    virtual ::grpc::Status Profile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::nvidia::inferenceserver::ProfileResponse* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::ProfileResponse>> AsyncProfile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::ProfileResponse>>(AsyncProfileRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::ProfileResponse>> PrepareAsyncProfile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::ProfileResponse>>(PrepareAsyncProfileRaw(context, request, cq));
    }
    // @@  .. cpp:var:: rpc Health(HealthRequest) returns (HealthResponse)
    // @@
    // @@     Check liveness and readiness of the inference server.
    // @@
    virtual ::grpc::Status Health(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::nvidia::inferenceserver::HealthResponse* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::HealthResponse>> AsyncHealth(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::HealthResponse>>(AsyncHealthRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::HealthResponse>> PrepareAsyncHealth(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::HealthResponse>>(PrepareAsyncHealthRaw(context, request, cq));
    }
    // @@  .. cpp:var:: rpc Infer(InferRequest) returns (InferResponse)
    // @@
    // @@     Request inference using a specific model. [ To handle large input
    // @@     tensors likely need to set the maximum message size to that they
    // @@     can be transmitted in one pass.
    // @@
    virtual ::grpc::Status Infer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::nvidia::inferenceserver::InferResponse* response) = 0;
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::InferResponse>> AsyncInfer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::InferResponse>>(AsyncInferRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::InferResponse>> PrepareAsyncInfer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::InferResponse>>(PrepareAsyncInferRaw(context, request, cq));
    }
    class async_interface {
     public:
      virtual ~async_interface() {}
      // @@  .. cpp:var:: rpc Status(StatusRequest) returns (StatusResponse)
      // @@
      // @@     Get status for entire inference server or for a specified model.
      // @@
      virtual void Status(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest* request, ::nvidia::inferenceserver::StatusResponse* response, std::function<void(::grpc::Status)>) = 0;
      virtual void Status(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest* request, ::nvidia::inferenceserver::StatusResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      // @@  .. cpp:var:: rpc Profile(ProfileRequest) returns (ProfileResponse)
      // @@
      // @@     Enable and disable low-level GPU profiling.
      // @@
      virtual void Profile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest* request, ::nvidia::inferenceserver::ProfileResponse* response, std::function<void(::grpc::Status)>) = 0;
      virtual void Profile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest* request, ::nvidia::inferenceserver::ProfileResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      // @@  .. cpp:var:: rpc Health(HealthRequest) returns (HealthResponse)
      // @@
      // @@     Check liveness and readiness of the inference server.
      // @@
      virtual void Health(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest* request, ::nvidia::inferenceserver::HealthResponse* response, std::function<void(::grpc::Status)>) = 0;
      virtual void Health(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest* request, ::nvidia::inferenceserver::HealthResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
      // @@  .. cpp:var:: rpc Infer(InferRequest) returns (InferResponse)
      // @@
      // @@     Request inference using a specific model. [ To handle large input
      // @@     tensors likely need to set the maximum message size to that they
      // @@     can be transmitted in one pass.
      // @@
      virtual void Infer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest* request, ::nvidia::inferenceserver::InferResponse* response, std::function<void(::grpc::Status)>) = 0;
      virtual void Infer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest* request, ::nvidia::inferenceserver::InferResponse* response, ::grpc::ClientUnaryReactor* reactor) = 0;
    };
    typedef class async_interface experimental_async_interface;
    virtual class async_interface* async() { return nullptr; }
    class async_interface* experimental_async() { return async(); }
   private:
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::StatusResponse>* AsyncStatusRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::StatusResponse>* PrepareAsyncStatusRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::ProfileResponse>* AsyncProfileRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::ProfileResponse>* PrepareAsyncProfileRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::HealthResponse>* AsyncHealthRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::HealthResponse>* PrepareAsyncHealthRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::InferResponse>* AsyncInferRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::grpc::CompletionQueue* cq) = 0;
    virtual ::grpc::ClientAsyncResponseReaderInterface< ::nvidia::inferenceserver::InferResponse>* PrepareAsyncInferRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::grpc::CompletionQueue* cq) = 0;
  };
  class Stub final : public StubInterface {
   public:
    Stub(const std::shared_ptr< ::grpc::ChannelInterface>& channel, const ::grpc::StubOptions& options = ::grpc::StubOptions());
    ::grpc::Status Status(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::nvidia::inferenceserver::StatusResponse* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::StatusResponse>> AsyncStatus(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::StatusResponse>>(AsyncStatusRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::StatusResponse>> PrepareAsyncStatus(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::StatusResponse>>(PrepareAsyncStatusRaw(context, request, cq));
    }
    ::grpc::Status Profile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::nvidia::inferenceserver::ProfileResponse* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::ProfileResponse>> AsyncProfile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::ProfileResponse>>(AsyncProfileRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::ProfileResponse>> PrepareAsyncProfile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::ProfileResponse>>(PrepareAsyncProfileRaw(context, request, cq));
    }
    ::grpc::Status Health(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::nvidia::inferenceserver::HealthResponse* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::HealthResponse>> AsyncHealth(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::HealthResponse>>(AsyncHealthRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::HealthResponse>> PrepareAsyncHealth(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::HealthResponse>>(PrepareAsyncHealthRaw(context, request, cq));
    }
    ::grpc::Status Infer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::nvidia::inferenceserver::InferResponse* response) override;
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::InferResponse>> AsyncInfer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::InferResponse>>(AsyncInferRaw(context, request, cq));
    }
    std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::InferResponse>> PrepareAsyncInfer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::grpc::CompletionQueue* cq) {
      return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::InferResponse>>(PrepareAsyncInferRaw(context, request, cq));
    }
    class async final :
      public StubInterface::async_interface {
     public:
      void Status(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest* request, ::nvidia::inferenceserver::StatusResponse* response, std::function<void(::grpc::Status)>) override;
      void Status(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest* request, ::nvidia::inferenceserver::StatusResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      void Profile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest* request, ::nvidia::inferenceserver::ProfileResponse* response, std::function<void(::grpc::Status)>) override;
      void Profile(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest* request, ::nvidia::inferenceserver::ProfileResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      void Health(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest* request, ::nvidia::inferenceserver::HealthResponse* response, std::function<void(::grpc::Status)>) override;
      void Health(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest* request, ::nvidia::inferenceserver::HealthResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
      void Infer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest* request, ::nvidia::inferenceserver::InferResponse* response, std::function<void(::grpc::Status)>) override;
      void Infer(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest* request, ::nvidia::inferenceserver::InferResponse* response, ::grpc::ClientUnaryReactor* reactor) override;
     private:
      friend class Stub;
      explicit async(Stub* stub): stub_(stub) { }
      Stub* stub() { return stub_; }
      Stub* stub_;
    };
    class async* async() override { return &async_stub_; }

   private:
    std::shared_ptr< ::grpc::ChannelInterface> channel_;
    class async async_stub_{this};
    ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::StatusResponse>* AsyncStatusRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::StatusResponse>* PrepareAsyncStatusRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::StatusRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::ProfileResponse>* AsyncProfileRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::ProfileResponse>* PrepareAsyncProfileRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::ProfileRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::HealthResponse>* AsyncHealthRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::HealthResponse>* PrepareAsyncHealthRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::HealthRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::InferResponse>* AsyncInferRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::grpc::CompletionQueue* cq) override;
    ::grpc::ClientAsyncResponseReader< ::nvidia::inferenceserver::InferResponse>* PrepareAsyncInferRaw(::grpc::ClientContext* context, const ::nvidia::inferenceserver::InferRequest& request, ::grpc::CompletionQueue* cq) override;
    const ::grpc::internal::RpcMethod rpcmethod_Status_;
    const ::grpc::internal::RpcMethod rpcmethod_Profile_;
    const ::grpc::internal::RpcMethod rpcmethod_Health_;
    const ::grpc::internal::RpcMethod rpcmethod_Infer_;
  };
  static std::unique_ptr<Stub> NewStub(const std::shared_ptr< ::grpc::ChannelInterface>& channel, const ::grpc::StubOptions& options = ::grpc::StubOptions());

  class Service : public ::grpc::Service {
   public:
    Service();
    virtual ~Service();
    // @@  .. cpp:var:: rpc Status(StatusRequest) returns (StatusResponse)
    // @@
    // @@     Get status for entire inference server or for a specified model.
    // @@
    virtual ::grpc::Status Status(::grpc::ServerContext* context, const ::nvidia::inferenceserver::StatusRequest* request, ::nvidia::inferenceserver::StatusResponse* response);
    // @@  .. cpp:var:: rpc Profile(ProfileRequest) returns (ProfileResponse)
    // @@
    // @@     Enable and disable low-level GPU profiling.
    // @@
    virtual ::grpc::Status Profile(::grpc::ServerContext* context, const ::nvidia::inferenceserver::ProfileRequest* request, ::nvidia::inferenceserver::ProfileResponse* response);
    // @@  .. cpp:var:: rpc Health(HealthRequest) returns (HealthResponse)
    // @@
    // @@     Check liveness and readiness of the inference server.
    // @@
    virtual ::grpc::Status Health(::grpc::ServerContext* context, const ::nvidia::inferenceserver::HealthRequest* request, ::nvidia::inferenceserver::HealthResponse* response);
    // @@  .. cpp:var:: rpc Infer(InferRequest) returns (InferResponse)
    // @@
    // @@     Request inference using a specific model. [ To handle large input
    // @@     tensors likely need to set the maximum message size to that they
    // @@     can be transmitted in one pass.
    // @@
    virtual ::grpc::Status Infer(::grpc::ServerContext* context, const ::nvidia::inferenceserver::InferRequest* request, ::nvidia::inferenceserver::InferResponse* response);
  };
  template <class BaseClass>
  class WithAsyncMethod_Status : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithAsyncMethod_Status() {
      ::grpc::Service::MarkMethodAsync(0);
    }
    ~WithAsyncMethod_Status() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Status(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::StatusRequest* /*request*/, ::nvidia::inferenceserver::StatusResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestStatus(::grpc::ServerContext* context, ::nvidia::inferenceserver::StatusRequest* request, ::grpc::ServerAsyncResponseWriter< ::nvidia::inferenceserver::StatusResponse>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(0, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithAsyncMethod_Profile : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithAsyncMethod_Profile() {
      ::grpc::Service::MarkMethodAsync(1);
    }
    ~WithAsyncMethod_Profile() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Profile(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::ProfileRequest* /*request*/, ::nvidia::inferenceserver::ProfileResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestProfile(::grpc::ServerContext* context, ::nvidia::inferenceserver::ProfileRequest* request, ::grpc::ServerAsyncResponseWriter< ::nvidia::inferenceserver::ProfileResponse>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(1, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithAsyncMethod_Health : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithAsyncMethod_Health() {
      ::grpc::Service::MarkMethodAsync(2);
    }
    ~WithAsyncMethod_Health() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Health(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::HealthRequest* /*request*/, ::nvidia::inferenceserver::HealthResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestHealth(::grpc::ServerContext* context, ::nvidia::inferenceserver::HealthRequest* request, ::grpc::ServerAsyncResponseWriter< ::nvidia::inferenceserver::HealthResponse>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(2, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithAsyncMethod_Infer : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithAsyncMethod_Infer() {
      ::grpc::Service::MarkMethodAsync(3);
    }
    ~WithAsyncMethod_Infer() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Infer(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::InferRequest* /*request*/, ::nvidia::inferenceserver::InferResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestInfer(::grpc::ServerContext* context, ::nvidia::inferenceserver::InferRequest* request, ::grpc::ServerAsyncResponseWriter< ::nvidia::inferenceserver::InferResponse>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(3, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  typedef WithAsyncMethod_Status<WithAsyncMethod_Profile<WithAsyncMethod_Health<WithAsyncMethod_Infer<Service > > > > AsyncService;
  template <class BaseClass>
  class WithCallbackMethod_Status : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithCallbackMethod_Status() {
      ::grpc::Service::MarkMethodCallback(0,
          new ::grpc::internal::CallbackUnaryHandler< ::nvidia::inferenceserver::StatusRequest, ::nvidia::inferenceserver::StatusResponse>(
            [this](
                   ::grpc::CallbackServerContext* context, const ::nvidia::inferenceserver::StatusRequest* request, ::nvidia::inferenceserver::StatusResponse* response) { return this->Status(context, request, response); }));}
    void SetMessageAllocatorFor_Status(
        ::grpc::MessageAllocator< ::nvidia::inferenceserver::StatusRequest, ::nvidia::inferenceserver::StatusResponse>* allocator) {
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::GetHandler(0);
      static_cast<::grpc::internal::CallbackUnaryHandler< ::nvidia::inferenceserver::StatusRequest, ::nvidia::inferenceserver::StatusResponse>*>(handler)
              ->SetMessageAllocator(allocator);
    }
    ~WithCallbackMethod_Status() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Status(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::StatusRequest* /*request*/, ::nvidia::inferenceserver::StatusResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    virtual ::grpc::ServerUnaryReactor* Status(
      ::grpc::CallbackServerContext* /*context*/, const ::nvidia::inferenceserver::StatusRequest* /*request*/, ::nvidia::inferenceserver::StatusResponse* /*response*/)  { return nullptr; }
  };
  template <class BaseClass>
  class WithCallbackMethod_Profile : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithCallbackMethod_Profile() {
      ::grpc::Service::MarkMethodCallback(1,
          new ::grpc::internal::CallbackUnaryHandler< ::nvidia::inferenceserver::ProfileRequest, ::nvidia::inferenceserver::ProfileResponse>(
            [this](
                   ::grpc::CallbackServerContext* context, const ::nvidia::inferenceserver::ProfileRequest* request, ::nvidia::inferenceserver::ProfileResponse* response) { return this->Profile(context, request, response); }));}
    void SetMessageAllocatorFor_Profile(
        ::grpc::MessageAllocator< ::nvidia::inferenceserver::ProfileRequest, ::nvidia::inferenceserver::ProfileResponse>* allocator) {
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::GetHandler(1);
      static_cast<::grpc::internal::CallbackUnaryHandler< ::nvidia::inferenceserver::ProfileRequest, ::nvidia::inferenceserver::ProfileResponse>*>(handler)
              ->SetMessageAllocator(allocator);
    }
    ~WithCallbackMethod_Profile() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Profile(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::ProfileRequest* /*request*/, ::nvidia::inferenceserver::ProfileResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    virtual ::grpc::ServerUnaryReactor* Profile(
      ::grpc::CallbackServerContext* /*context*/, const ::nvidia::inferenceserver::ProfileRequest* /*request*/, ::nvidia::inferenceserver::ProfileResponse* /*response*/)  { return nullptr; }
  };
  template <class BaseClass>
  class WithCallbackMethod_Health : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithCallbackMethod_Health() {
      ::grpc::Service::MarkMethodCallback(2,
          new ::grpc::internal::CallbackUnaryHandler< ::nvidia::inferenceserver::HealthRequest, ::nvidia::inferenceserver::HealthResponse>(
            [this](
                   ::grpc::CallbackServerContext* context, const ::nvidia::inferenceserver::HealthRequest* request, ::nvidia::inferenceserver::HealthResponse* response) { return this->Health(context, request, response); }));}
    void SetMessageAllocatorFor_Health(
        ::grpc::MessageAllocator< ::nvidia::inferenceserver::HealthRequest, ::nvidia::inferenceserver::HealthResponse>* allocator) {
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::GetHandler(2);
      static_cast<::grpc::internal::CallbackUnaryHandler< ::nvidia::inferenceserver::HealthRequest, ::nvidia::inferenceserver::HealthResponse>*>(handler)
              ->SetMessageAllocator(allocator);
    }
    ~WithCallbackMethod_Health() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Health(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::HealthRequest* /*request*/, ::nvidia::inferenceserver::HealthResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    virtual ::grpc::ServerUnaryReactor* Health(
      ::grpc::CallbackServerContext* /*context*/, const ::nvidia::inferenceserver::HealthRequest* /*request*/, ::nvidia::inferenceserver::HealthResponse* /*response*/)  { return nullptr; }
  };
  template <class BaseClass>
  class WithCallbackMethod_Infer : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithCallbackMethod_Infer() {
      ::grpc::Service::MarkMethodCallback(3,
          new ::grpc::internal::CallbackUnaryHandler< ::nvidia::inferenceserver::InferRequest, ::nvidia::inferenceserver::InferResponse>(
            [this](
                   ::grpc::CallbackServerContext* context, const ::nvidia::inferenceserver::InferRequest* request, ::nvidia::inferenceserver::InferResponse* response) { return this->Infer(context, request, response); }));}
    void SetMessageAllocatorFor_Infer(
        ::grpc::MessageAllocator< ::nvidia::inferenceserver::InferRequest, ::nvidia::inferenceserver::InferResponse>* allocator) {
      ::grpc::internal::MethodHandler* const handler = ::grpc::Service::GetHandler(3);
      static_cast<::grpc::internal::CallbackUnaryHandler< ::nvidia::inferenceserver::InferRequest, ::nvidia::inferenceserver::InferResponse>*>(handler)
              ->SetMessageAllocator(allocator);
    }
    ~WithCallbackMethod_Infer() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Infer(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::InferRequest* /*request*/, ::nvidia::inferenceserver::InferResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    virtual ::grpc::ServerUnaryReactor* Infer(
      ::grpc::CallbackServerContext* /*context*/, const ::nvidia::inferenceserver::InferRequest* /*request*/, ::nvidia::inferenceserver::InferResponse* /*response*/)  { return nullptr; }
  };
  typedef WithCallbackMethod_Status<WithCallbackMethod_Profile<WithCallbackMethod_Health<WithCallbackMethod_Infer<Service > > > > CallbackService;
  typedef CallbackService ExperimentalCallbackService;
  template <class BaseClass>
  class WithGenericMethod_Status : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithGenericMethod_Status() {
      ::grpc::Service::MarkMethodGeneric(0);
    }
    ~WithGenericMethod_Status() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Status(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::StatusRequest* /*request*/, ::nvidia::inferenceserver::StatusResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithGenericMethod_Profile : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithGenericMethod_Profile() {
      ::grpc::Service::MarkMethodGeneric(1);
    }
    ~WithGenericMethod_Profile() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Profile(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::ProfileRequest* /*request*/, ::nvidia::inferenceserver::ProfileResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithGenericMethod_Health : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithGenericMethod_Health() {
      ::grpc::Service::MarkMethodGeneric(2);
    }
    ~WithGenericMethod_Health() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Health(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::HealthRequest* /*request*/, ::nvidia::inferenceserver::HealthResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithGenericMethod_Infer : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithGenericMethod_Infer() {
      ::grpc::Service::MarkMethodGeneric(3);
    }
    ~WithGenericMethod_Infer() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Infer(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::InferRequest* /*request*/, ::nvidia::inferenceserver::InferResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
  };
  template <class BaseClass>
  class WithRawMethod_Status : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawMethod_Status() {
      ::grpc::Service::MarkMethodRaw(0);
    }
    ~WithRawMethod_Status() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Status(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::StatusRequest* /*request*/, ::nvidia::inferenceserver::StatusResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestStatus(::grpc::ServerContext* context, ::grpc::ByteBuffer* request, ::grpc::ServerAsyncResponseWriter< ::grpc::ByteBuffer>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(0, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithRawMethod_Profile : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawMethod_Profile() {
      ::grpc::Service::MarkMethodRaw(1);
    }
    ~WithRawMethod_Profile() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Profile(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::ProfileRequest* /*request*/, ::nvidia::inferenceserver::ProfileResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestProfile(::grpc::ServerContext* context, ::grpc::ByteBuffer* request, ::grpc::ServerAsyncResponseWriter< ::grpc::ByteBuffer>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(1, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithRawMethod_Health : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawMethod_Health() {
      ::grpc::Service::MarkMethodRaw(2);
    }
    ~WithRawMethod_Health() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Health(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::HealthRequest* /*request*/, ::nvidia::inferenceserver::HealthResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestHealth(::grpc::ServerContext* context, ::grpc::ByteBuffer* request, ::grpc::ServerAsyncResponseWriter< ::grpc::ByteBuffer>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(2, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithRawMethod_Infer : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawMethod_Infer() {
      ::grpc::Service::MarkMethodRaw(3);
    }
    ~WithRawMethod_Infer() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Infer(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::InferRequest* /*request*/, ::nvidia::inferenceserver::InferResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    void RequestInfer(::grpc::ServerContext* context, ::grpc::ByteBuffer* request, ::grpc::ServerAsyncResponseWriter< ::grpc::ByteBuffer>* response, ::grpc::CompletionQueue* new_call_cq, ::grpc::ServerCompletionQueue* notification_cq, void *tag) {
      ::grpc::Service::RequestAsyncUnary(3, context, request, response, new_call_cq, notification_cq, tag);
    }
  };
  template <class BaseClass>
  class WithRawCallbackMethod_Status : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawCallbackMethod_Status() {
      ::grpc::Service::MarkMethodRawCallback(0,
          new ::grpc::internal::CallbackUnaryHandler< ::grpc::ByteBuffer, ::grpc::ByteBuffer>(
            [this](
                   ::grpc::CallbackServerContext* context, const ::grpc::ByteBuffer* request, ::grpc::ByteBuffer* response) { return this->Status(context, request, response); }));
    }
    ~WithRawCallbackMethod_Status() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Status(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::StatusRequest* /*request*/, ::nvidia::inferenceserver::StatusResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    virtual ::grpc::ServerUnaryReactor* Status(
      ::grpc::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)  { return nullptr; }
  };
  template <class BaseClass>
  class WithRawCallbackMethod_Profile : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawCallbackMethod_Profile() {
      ::grpc::Service::MarkMethodRawCallback(1,
          new ::grpc::internal::CallbackUnaryHandler< ::grpc::ByteBuffer, ::grpc::ByteBuffer>(
            [this](
                   ::grpc::CallbackServerContext* context, const ::grpc::ByteBuffer* request, ::grpc::ByteBuffer* response) { return this->Profile(context, request, response); }));
    }
    ~WithRawCallbackMethod_Profile() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Profile(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::ProfileRequest* /*request*/, ::nvidia::inferenceserver::ProfileResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    virtual ::grpc::ServerUnaryReactor* Profile(
      ::grpc::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)  { return nullptr; }
  };
  template <class BaseClass>
  class WithRawCallbackMethod_Health : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawCallbackMethod_Health() {
      ::grpc::Service::MarkMethodRawCallback(2,
          new ::grpc::internal::CallbackUnaryHandler< ::grpc::ByteBuffer, ::grpc::ByteBuffer>(
            [this](
                   ::grpc::CallbackServerContext* context, const ::grpc::ByteBuffer* request, ::grpc::ByteBuffer* response) { return this->Health(context, request, response); }));
    }
    ~WithRawCallbackMethod_Health() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Health(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::HealthRequest* /*request*/, ::nvidia::inferenceserver::HealthResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    virtual ::grpc::ServerUnaryReactor* Health(
      ::grpc::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)  { return nullptr; }
  };
  template <class BaseClass>
  class WithRawCallbackMethod_Infer : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithRawCallbackMethod_Infer() {
      ::grpc::Service::MarkMethodRawCallback(3,
          new ::grpc::internal::CallbackUnaryHandler< ::grpc::ByteBuffer, ::grpc::ByteBuffer>(
            [this](
                   ::grpc::CallbackServerContext* context, const ::grpc::ByteBuffer* request, ::grpc::ByteBuffer* response) { return this->Infer(context, request, response); }));
    }
    ~WithRawCallbackMethod_Infer() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable synchronous version of this method
    ::grpc::Status Infer(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::InferRequest* /*request*/, ::nvidia::inferenceserver::InferResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    virtual ::grpc::ServerUnaryReactor* Infer(
      ::grpc::CallbackServerContext* /*context*/, const ::grpc::ByteBuffer* /*request*/, ::grpc::ByteBuffer* /*response*/)  { return nullptr; }
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_Status : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithStreamedUnaryMethod_Status() {
      ::grpc::Service::MarkMethodStreamed(0,
        new ::grpc::internal::StreamedUnaryHandler<
          ::nvidia::inferenceserver::StatusRequest, ::nvidia::inferenceserver::StatusResponse>(
            [this](::grpc::ServerContext* context,
                   ::grpc::ServerUnaryStreamer<
                     ::nvidia::inferenceserver::StatusRequest, ::nvidia::inferenceserver::StatusResponse>* streamer) {
                       return this->StreamedStatus(context,
                         streamer);
                  }));
    }
    ~WithStreamedUnaryMethod_Status() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status Status(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::StatusRequest* /*request*/, ::nvidia::inferenceserver::StatusResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedStatus(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::nvidia::inferenceserver::StatusRequest,::nvidia::inferenceserver::StatusResponse>* server_unary_streamer) = 0;
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_Profile : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithStreamedUnaryMethod_Profile() {
      ::grpc::Service::MarkMethodStreamed(1,
        new ::grpc::internal::StreamedUnaryHandler<
          ::nvidia::inferenceserver::ProfileRequest, ::nvidia::inferenceserver::ProfileResponse>(
            [this](::grpc::ServerContext* context,
                   ::grpc::ServerUnaryStreamer<
                     ::nvidia::inferenceserver::ProfileRequest, ::nvidia::inferenceserver::ProfileResponse>* streamer) {
                       return this->StreamedProfile(context,
                         streamer);
                  }));
    }
    ~WithStreamedUnaryMethod_Profile() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status Profile(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::ProfileRequest* /*request*/, ::nvidia::inferenceserver::ProfileResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedProfile(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::nvidia::inferenceserver::ProfileRequest,::nvidia::inferenceserver::ProfileResponse>* server_unary_streamer) = 0;
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_Health : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithStreamedUnaryMethod_Health() {
      ::grpc::Service::MarkMethodStreamed(2,
        new ::grpc::internal::StreamedUnaryHandler<
          ::nvidia::inferenceserver::HealthRequest, ::nvidia::inferenceserver::HealthResponse>(
            [this](::grpc::ServerContext* context,
                   ::grpc::ServerUnaryStreamer<
                     ::nvidia::inferenceserver::HealthRequest, ::nvidia::inferenceserver::HealthResponse>* streamer) {
                       return this->StreamedHealth(context,
                         streamer);
                  }));
    }
    ~WithStreamedUnaryMethod_Health() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status Health(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::HealthRequest* /*request*/, ::nvidia::inferenceserver::HealthResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedHealth(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::nvidia::inferenceserver::HealthRequest,::nvidia::inferenceserver::HealthResponse>* server_unary_streamer) = 0;
  };
  template <class BaseClass>
  class WithStreamedUnaryMethod_Infer : public BaseClass {
   private:
    void BaseClassMustBeDerivedFromService(const Service* /*service*/) {}
   public:
    WithStreamedUnaryMethod_Infer() {
      ::grpc::Service::MarkMethodStreamed(3,
        new ::grpc::internal::StreamedUnaryHandler<
          ::nvidia::inferenceserver::InferRequest, ::nvidia::inferenceserver::InferResponse>(
            [this](::grpc::ServerContext* context,
                   ::grpc::ServerUnaryStreamer<
                     ::nvidia::inferenceserver::InferRequest, ::nvidia::inferenceserver::InferResponse>* streamer) {
                       return this->StreamedInfer(context,
                         streamer);
                  }));
    }
    ~WithStreamedUnaryMethod_Infer() override {
      BaseClassMustBeDerivedFromService(this);
    }
    // disable regular version of this method
    ::grpc::Status Infer(::grpc::ServerContext* /*context*/, const ::nvidia::inferenceserver::InferRequest* /*request*/, ::nvidia::inferenceserver::InferResponse* /*response*/) override {
      abort();
      return ::grpc::Status(::grpc::StatusCode::UNIMPLEMENTED, "");
    }
    // replace default version of method with streamed unary
    virtual ::grpc::Status StreamedInfer(::grpc::ServerContext* context, ::grpc::ServerUnaryStreamer< ::nvidia::inferenceserver::InferRequest,::nvidia::inferenceserver::InferResponse>* server_unary_streamer) = 0;
  };
  typedef WithStreamedUnaryMethod_Status<WithStreamedUnaryMethod_Profile<WithStreamedUnaryMethod_Health<WithStreamedUnaryMethod_Infer<Service > > > > StreamedUnaryService;
  typedef Service SplitStreamedService;
  typedef WithStreamedUnaryMethod_Status<WithStreamedUnaryMethod_Profile<WithStreamedUnaryMethod_Health<WithStreamedUnaryMethod_Infer<Service > > > > StreamedService;
};

}  // namespace inferenceserver
}  // namespace nvidia


#endif  // GRPC_grpc_5fservice_2eproto__INCLUDED
