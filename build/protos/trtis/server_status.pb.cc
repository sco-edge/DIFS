// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: server_status.proto

#include "server_status.pb.h"

#include <algorithm>

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/wire_format_lite.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>

PROTOBUF_PRAGMA_INIT_SEG
namespace nvidia {
namespace inferenceserver {
constexpr StatDuration::StatDuration(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : count_(uint64_t{0u})
  , total_time_ns_(uint64_t{0u}){}
struct StatDurationDefaultTypeInternal {
  constexpr StatDurationDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~StatDurationDefaultTypeInternal() {}
  union {
    StatDuration _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT StatDurationDefaultTypeInternal _StatDuration_default_instance_;
constexpr StatusRequestStats::StatusRequestStats(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : success_(nullptr){}
struct StatusRequestStatsDefaultTypeInternal {
  constexpr StatusRequestStatsDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~StatusRequestStatsDefaultTypeInternal() {}
  union {
    StatusRequestStats _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT StatusRequestStatsDefaultTypeInternal _StatusRequestStats_default_instance_;
constexpr ProfileRequestStats::ProfileRequestStats(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : success_(nullptr){}
struct ProfileRequestStatsDefaultTypeInternal {
  constexpr ProfileRequestStatsDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~ProfileRequestStatsDefaultTypeInternal() {}
  union {
    ProfileRequestStats _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT ProfileRequestStatsDefaultTypeInternal _ProfileRequestStats_default_instance_;
constexpr HealthRequestStats::HealthRequestStats(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : success_(nullptr){}
struct HealthRequestStatsDefaultTypeInternal {
  constexpr HealthRequestStatsDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~HealthRequestStatsDefaultTypeInternal() {}
  union {
    HealthRequestStats _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT HealthRequestStatsDefaultTypeInternal _HealthRequestStats_default_instance_;
constexpr InferRequestStats::InferRequestStats(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : success_(nullptr)
  , failed_(nullptr)
  , compute_(nullptr)
  , queue_(nullptr){}
struct InferRequestStatsDefaultTypeInternal {
  constexpr InferRequestStatsDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~InferRequestStatsDefaultTypeInternal() {}
  union {
    InferRequestStats _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT InferRequestStatsDefaultTypeInternal _InferRequestStats_default_instance_;
constexpr ModelVersionStatus_InferStatsEntry_DoNotUse::ModelVersionStatus_InferStatsEntry_DoNotUse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized){}
struct ModelVersionStatus_InferStatsEntry_DoNotUseDefaultTypeInternal {
  constexpr ModelVersionStatus_InferStatsEntry_DoNotUseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~ModelVersionStatus_InferStatsEntry_DoNotUseDefaultTypeInternal() {}
  union {
    ModelVersionStatus_InferStatsEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT ModelVersionStatus_InferStatsEntry_DoNotUseDefaultTypeInternal _ModelVersionStatus_InferStatsEntry_DoNotUse_default_instance_;
constexpr ModelVersionStatus::ModelVersionStatus(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : infer_stats_(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{})
  , model_execution_count_(uint64_t{0u})
  , model_inference_count_(uint64_t{0u})
  , ready_state_(0)
{}
struct ModelVersionStatusDefaultTypeInternal {
  constexpr ModelVersionStatusDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~ModelVersionStatusDefaultTypeInternal() {}
  union {
    ModelVersionStatus _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT ModelVersionStatusDefaultTypeInternal _ModelVersionStatus_default_instance_;
constexpr ModelStatus_VersionStatusEntry_DoNotUse::ModelStatus_VersionStatusEntry_DoNotUse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized){}
struct ModelStatus_VersionStatusEntry_DoNotUseDefaultTypeInternal {
  constexpr ModelStatus_VersionStatusEntry_DoNotUseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~ModelStatus_VersionStatusEntry_DoNotUseDefaultTypeInternal() {}
  union {
    ModelStatus_VersionStatusEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT ModelStatus_VersionStatusEntry_DoNotUseDefaultTypeInternal _ModelStatus_VersionStatusEntry_DoNotUse_default_instance_;
constexpr ModelStatus::ModelStatus(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : version_status_(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{})
  , config_(nullptr){}
struct ModelStatusDefaultTypeInternal {
  constexpr ModelStatusDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~ModelStatusDefaultTypeInternal() {}
  union {
    ModelStatus _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT ModelStatusDefaultTypeInternal _ModelStatus_default_instance_;
constexpr ServerStatus_ModelStatusEntry_DoNotUse::ServerStatus_ModelStatusEntry_DoNotUse(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized){}
struct ServerStatus_ModelStatusEntry_DoNotUseDefaultTypeInternal {
  constexpr ServerStatus_ModelStatusEntry_DoNotUseDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~ServerStatus_ModelStatusEntry_DoNotUseDefaultTypeInternal() {}
  union {
    ServerStatus_ModelStatusEntry_DoNotUse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT ServerStatus_ModelStatusEntry_DoNotUseDefaultTypeInternal _ServerStatus_ModelStatusEntry_DoNotUse_default_instance_;
constexpr ServerStatus::ServerStatus(
  ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized)
  : model_status_(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{})
  , id_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , version_(&::PROTOBUF_NAMESPACE_ID::internal::fixed_address_empty_string)
  , status_stats_(nullptr)
  , profile_stats_(nullptr)
  , health_stats_(nullptr)
  , uptime_ns_(uint64_t{0u})
  , ready_state_(0)
{}
struct ServerStatusDefaultTypeInternal {
  constexpr ServerStatusDefaultTypeInternal()
    : _instance(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized{}) {}
  ~ServerStatusDefaultTypeInternal() {}
  union {
    ServerStatus _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT ServerStatusDefaultTypeInternal _ServerStatus_default_instance_;
}  // namespace inferenceserver
}  // namespace nvidia
static ::PROTOBUF_NAMESPACE_ID::Metadata file_level_metadata_server_5fstatus_2eproto[11];
static const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* file_level_enum_descriptors_server_5fstatus_2eproto[2];
static constexpr ::PROTOBUF_NAMESPACE_ID::ServiceDescriptor const** file_level_service_descriptors_server_5fstatus_2eproto = nullptr;

const uint32_t TableStruct_server_5fstatus_2eproto::offsets[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::StatDuration, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::StatDuration, count_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::StatDuration, total_time_ns_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::StatusRequestStats, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::StatusRequestStats, success_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ProfileRequestStats, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ProfileRequestStats, success_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::HealthRequestStats, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::HealthRequestStats, success_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::InferRequestStats, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::InferRequestStats, success_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::InferRequestStats, failed_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::InferRequestStats, compute_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::InferRequestStats, queue_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelVersionStatus, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelVersionStatus, ready_state_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelVersionStatus, infer_stats_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelVersionStatus, model_execution_count_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelVersionStatus, model_inference_count_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelStatus, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelStatus, config_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ModelStatus, version_status_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse, _has_bits_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse, key_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse, value_),
  0,
  1,
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus, id_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus, version_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus, ready_state_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus, uptime_ns_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus, model_status_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus, status_stats_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus, profile_stats_),
  PROTOBUF_FIELD_OFFSET(::nvidia::inferenceserver::ServerStatus, health_stats_),
};
static const ::PROTOBUF_NAMESPACE_ID::internal::MigrationSchema schemas[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  { 0, -1, -1, sizeof(::nvidia::inferenceserver::StatDuration)},
  { 8, -1, -1, sizeof(::nvidia::inferenceserver::StatusRequestStats)},
  { 15, -1, -1, sizeof(::nvidia::inferenceserver::ProfileRequestStats)},
  { 22, -1, -1, sizeof(::nvidia::inferenceserver::HealthRequestStats)},
  { 29, -1, -1, sizeof(::nvidia::inferenceserver::InferRequestStats)},
  { 39, 47, -1, sizeof(::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse)},
  { 49, -1, -1, sizeof(::nvidia::inferenceserver::ModelVersionStatus)},
  { 59, 67, -1, sizeof(::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse)},
  { 69, -1, -1, sizeof(::nvidia::inferenceserver::ModelStatus)},
  { 77, 85, -1, sizeof(::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse)},
  { 87, -1, -1, sizeof(::nvidia::inferenceserver::ServerStatus)},
};

static ::PROTOBUF_NAMESPACE_ID::Message const * const file_default_instances[] = {
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_StatDuration_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_StatusRequestStats_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_ProfileRequestStats_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_HealthRequestStats_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_InferRequestStats_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_ModelVersionStatus_InferStatsEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_ModelVersionStatus_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_ModelStatus_VersionStatusEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_ModelStatus_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_ServerStatus_ModelStatusEntry_DoNotUse_default_instance_),
  reinterpret_cast<const ::PROTOBUF_NAMESPACE_ID::Message*>(&::nvidia::inferenceserver::_ServerStatus_default_instance_),
};

const char descriptor_table_protodef_server_5fstatus_2eproto[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) =
  "\n\023server_status.proto\022\026nvidia.inferences"
  "erver\032\022model_config.proto\"4\n\014StatDuratio"
  "n\022\r\n\005count\030\001 \001(\004\022\025\n\rtotal_time_ns\030\002 \001(\004\""
  "K\n\022StatusRequestStats\0225\n\007success\030\001 \001(\0132$"
  ".nvidia.inferenceserver.StatDuration\"L\n\023"
  "ProfileRequestStats\0225\n\007success\030\001 \001(\0132$.n"
  "vidia.inferenceserver.StatDuration\"K\n\022He"
  "althRequestStats\0225\n\007success\030\001 \001(\0132$.nvid"
  "ia.inferenceserver.StatDuration\"\354\001\n\021Infe"
  "rRequestStats\0225\n\007success\030\001 \001(\0132$.nvidia."
  "inferenceserver.StatDuration\0224\n\006failed\030\002"
  " \001(\0132$.nvidia.inferenceserver.StatDurati"
  "on\0225\n\007compute\030\003 \001(\0132$.nvidia.inferencese"
  "rver.StatDuration\0223\n\005queue\030\004 \001(\0132$.nvidi"
  "a.inferenceserver.StatDuration\"\277\002\n\022Model"
  "VersionStatus\022<\n\013ready_state\030\001 \001(\0162\'.nvi"
  "dia.inferenceserver.ModelReadyState\022O\n\013i"
  "nfer_stats\030\002 \003(\0132:.nvidia.inferenceserve"
  "r.ModelVersionStatus.InferStatsEntry\022\035\n\025"
  "model_execution_count\030\003 \001(\004\022\035\n\025model_inf"
  "erence_count\030\004 \001(\004\032\\\n\017InferStatsEntry\022\013\n"
  "\003key\030\001 \001(\r\0228\n\005value\030\002 \001(\0132).nvidia.infer"
  "enceserver.InferRequestStats:\0028\001\"\364\001\n\013Mod"
  "elStatus\0223\n\006config\030\001 \001(\0132#.nvidia.infere"
  "nceserver.ModelConfig\022N\n\016version_status\030"
  "\002 \003(\01326.nvidia.inferenceserver.ModelStat"
  "us.VersionStatusEntry\032`\n\022VersionStatusEn"
  "try\022\013\n\003key\030\001 \001(\003\0229\n\005value\030\002 \001(\0132*.nvidia"
  ".inferenceserver.ModelVersionStatus:\0028\001\""
  "\353\003\n\014ServerStatus\022\n\n\002id\030\001 \001(\t\022\017\n\007version\030"
  "\002 \001(\t\022=\n\013ready_state\030\007 \001(\0162(.nvidia.infe"
  "renceserver.ServerReadyState\022\021\n\tuptime_n"
  "s\030\003 \001(\004\022K\n\014model_status\030\004 \003(\01325.nvidia.i"
  "nferenceserver.ServerStatus.ModelStatusE"
  "ntry\022@\n\014status_stats\030\005 \001(\0132*.nvidia.infe"
  "renceserver.StatusRequestStats\022B\n\rprofil"
  "e_stats\030\006 \001(\0132+.nvidia.inferenceserver.P"
  "rofileRequestStats\022@\n\014health_stats\030\010 \001(\013"
  "2*.nvidia.inferenceserver.HealthRequestS"
  "tats\032W\n\020ModelStatusEntry\022\013\n\003key\030\001 \001(\t\0222\n"
  "\005value\030\002 \001(\0132#.nvidia.inferenceserver.Mo"
  "delStatus:\0028\001*t\n\017ModelReadyState\022\021\n\rMODE"
  "L_UNKNOWN\020\000\022\017\n\013MODEL_READY\020\001\022\025\n\021MODEL_UN"
  "AVAILABLE\020\002\022\021\n\rMODEL_LOADING\020\003\022\023\n\017MODEL_"
  "UNLOADING\020\004*\206\001\n\020ServerReadyState\022\022\n\016SERV"
  "ER_INVALID\020\000\022\027\n\023SERVER_INITIALIZING\020\001\022\020\n"
  "\014SERVER_READY\020\002\022\022\n\016SERVER_EXITING\020\003\022\037\n\033S"
  "ERVER_FAILED_TO_INITIALIZE\020\nb\006proto3"
  ;
static const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable*const descriptor_table_server_5fstatus_2eproto_deps[1] = {
  &::descriptor_table_model_5fconfig_2eproto,
};
static ::PROTOBUF_NAMESPACE_ID::internal::once_flag descriptor_table_server_5fstatus_2eproto_once;
const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_server_5fstatus_2eproto = {
  false, false, 1916, descriptor_table_protodef_server_5fstatus_2eproto, "server_status.proto", 
  &descriptor_table_server_5fstatus_2eproto_once, descriptor_table_server_5fstatus_2eproto_deps, 1, 11,
  schemas, file_default_instances, TableStruct_server_5fstatus_2eproto::offsets,
  file_level_metadata_server_5fstatus_2eproto, file_level_enum_descriptors_server_5fstatus_2eproto, file_level_service_descriptors_server_5fstatus_2eproto,
};
PROTOBUF_ATTRIBUTE_WEAK const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable* descriptor_table_server_5fstatus_2eproto_getter() {
  return &descriptor_table_server_5fstatus_2eproto;
}

// Force running AddDescriptors() at dynamic initialization time.
PROTOBUF_ATTRIBUTE_INIT_PRIORITY static ::PROTOBUF_NAMESPACE_ID::internal::AddDescriptorsRunner dynamic_init_dummy_server_5fstatus_2eproto(&descriptor_table_server_5fstatus_2eproto);
namespace nvidia {
namespace inferenceserver {
const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* ModelReadyState_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_server_5fstatus_2eproto);
  return file_level_enum_descriptors_server_5fstatus_2eproto[0];
}
bool ModelReadyState_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
      return true;
    default:
      return false;
  }
}

const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* ServerReadyState_descriptor() {
  ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&descriptor_table_server_5fstatus_2eproto);
  return file_level_enum_descriptors_server_5fstatus_2eproto[1];
}
bool ServerReadyState_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 10:
      return true;
    default:
      return false;
  }
}


// ===================================================================

class StatDuration::_Internal {
 public:
};

StatDuration::StatDuration(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:nvidia.inferenceserver.StatDuration)
}
StatDuration::StatDuration(const StatDuration& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  ::memcpy(&count_, &from.count_,
    static_cast<size_t>(reinterpret_cast<char*>(&total_time_ns_) -
    reinterpret_cast<char*>(&count_)) + sizeof(total_time_ns_));
  // @@protoc_insertion_point(copy_constructor:nvidia.inferenceserver.StatDuration)
}

inline void StatDuration::SharedCtor() {
::memset(reinterpret_cast<char*>(this) + static_cast<size_t>(
    reinterpret_cast<char*>(&count_) - reinterpret_cast<char*>(this)),
    0, static_cast<size_t>(reinterpret_cast<char*>(&total_time_ns_) -
    reinterpret_cast<char*>(&count_)) + sizeof(total_time_ns_));
}

StatDuration::~StatDuration() {
  // @@protoc_insertion_point(destructor:nvidia.inferenceserver.StatDuration)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void StatDuration::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
}

void StatDuration::ArenaDtor(void* object) {
  StatDuration* _this = reinterpret_cast< StatDuration* >(object);
  (void)_this;
}
void StatDuration::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena*) {
}
void StatDuration::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void StatDuration::Clear() {
// @@protoc_insertion_point(message_clear_start:nvidia.inferenceserver.StatDuration)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  ::memset(&count_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&total_time_ns_) -
      reinterpret_cast<char*>(&count_)) + sizeof(total_time_ns_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* StatDuration::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // uint64 count = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          count_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // uint64 total_time_ns = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 16)) {
          total_time_ns_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* StatDuration::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:nvidia.inferenceserver.StatDuration)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // uint64 count = 1;
  if (this->_internal_count() != 0) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::WriteUInt64ToArray(1, this->_internal_count(), target);
  }

  // uint64 total_time_ns = 2;
  if (this->_internal_total_time_ns() != 0) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::WriteUInt64ToArray(2, this->_internal_total_time_ns(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:nvidia.inferenceserver.StatDuration)
  return target;
}

size_t StatDuration::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:nvidia.inferenceserver.StatDuration)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // uint64 count = 1;
  if (this->_internal_count() != 0) {
    total_size += ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::UInt64SizePlusOne(this->_internal_count());
  }

  // uint64 total_time_ns = 2;
  if (this->_internal_total_time_ns() != 0) {
    total_size += ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::UInt64SizePlusOne(this->_internal_total_time_ns());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData StatDuration::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    StatDuration::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*StatDuration::GetClassData() const { return &_class_data_; }

void StatDuration::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<StatDuration *>(to)->MergeFrom(
      static_cast<const StatDuration &>(from));
}


void StatDuration::MergeFrom(const StatDuration& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:nvidia.inferenceserver.StatDuration)
  GOOGLE_DCHECK_NE(&from, this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_count() != 0) {
    _internal_set_count(from._internal_count());
  }
  if (from._internal_total_time_ns() != 0) {
    _internal_set_total_time_ns(from._internal_total_time_ns());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void StatDuration::CopyFrom(const StatDuration& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:nvidia.inferenceserver.StatDuration)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool StatDuration::IsInitialized() const {
  return true;
}

void StatDuration::InternalSwap(StatDuration* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(StatDuration, total_time_ns_)
      + sizeof(StatDuration::total_time_ns_)
      - PROTOBUF_FIELD_OFFSET(StatDuration, count_)>(
          reinterpret_cast<char*>(&count_),
          reinterpret_cast<char*>(&other->count_));
}

::PROTOBUF_NAMESPACE_ID::Metadata StatDuration::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[0]);
}

// ===================================================================

class StatusRequestStats::_Internal {
 public:
  static const ::nvidia::inferenceserver::StatDuration& success(const StatusRequestStats* msg);
};

const ::nvidia::inferenceserver::StatDuration&
StatusRequestStats::_Internal::success(const StatusRequestStats* msg) {
  return *msg->success_;
}
StatusRequestStats::StatusRequestStats(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:nvidia.inferenceserver.StatusRequestStats)
}
StatusRequestStats::StatusRequestStats(const StatusRequestStats& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_success()) {
    success_ = new ::nvidia::inferenceserver::StatDuration(*from.success_);
  } else {
    success_ = nullptr;
  }
  // @@protoc_insertion_point(copy_constructor:nvidia.inferenceserver.StatusRequestStats)
}

inline void StatusRequestStats::SharedCtor() {
success_ = nullptr;
}

StatusRequestStats::~StatusRequestStats() {
  // @@protoc_insertion_point(destructor:nvidia.inferenceserver.StatusRequestStats)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void StatusRequestStats::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete success_;
}

void StatusRequestStats::ArenaDtor(void* object) {
  StatusRequestStats* _this = reinterpret_cast< StatusRequestStats* >(object);
  (void)_this;
}
void StatusRequestStats::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena*) {
}
void StatusRequestStats::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void StatusRequestStats::Clear() {
// @@protoc_insertion_point(message_clear_start:nvidia.inferenceserver.StatusRequestStats)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaForAllocation() == nullptr && success_ != nullptr) {
    delete success_;
  }
  success_ = nullptr;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* StatusRequestStats::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .nvidia.inferenceserver.StatDuration success = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr = ctx->ParseMessage(_internal_mutable_success(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* StatusRequestStats::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:nvidia.inferenceserver.StatusRequestStats)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .nvidia.inferenceserver.StatDuration success = 1;
  if (this->_internal_has_success()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        1, _Internal::success(this), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:nvidia.inferenceserver.StatusRequestStats)
  return target;
}

size_t StatusRequestStats::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:nvidia.inferenceserver.StatusRequestStats)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .nvidia.inferenceserver.StatDuration success = 1;
  if (this->_internal_has_success()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *success_);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData StatusRequestStats::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    StatusRequestStats::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*StatusRequestStats::GetClassData() const { return &_class_data_; }

void StatusRequestStats::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<StatusRequestStats *>(to)->MergeFrom(
      static_cast<const StatusRequestStats &>(from));
}


void StatusRequestStats::MergeFrom(const StatusRequestStats& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:nvidia.inferenceserver.StatusRequestStats)
  GOOGLE_DCHECK_NE(&from, this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_has_success()) {
    _internal_mutable_success()->::nvidia::inferenceserver::StatDuration::MergeFrom(from._internal_success());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void StatusRequestStats::CopyFrom(const StatusRequestStats& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:nvidia.inferenceserver.StatusRequestStats)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool StatusRequestStats::IsInitialized() const {
  return true;
}

void StatusRequestStats::InternalSwap(StatusRequestStats* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  swap(success_, other->success_);
}

::PROTOBUF_NAMESPACE_ID::Metadata StatusRequestStats::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[1]);
}

// ===================================================================

class ProfileRequestStats::_Internal {
 public:
  static const ::nvidia::inferenceserver::StatDuration& success(const ProfileRequestStats* msg);
};

const ::nvidia::inferenceserver::StatDuration&
ProfileRequestStats::_Internal::success(const ProfileRequestStats* msg) {
  return *msg->success_;
}
ProfileRequestStats::ProfileRequestStats(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:nvidia.inferenceserver.ProfileRequestStats)
}
ProfileRequestStats::ProfileRequestStats(const ProfileRequestStats& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_success()) {
    success_ = new ::nvidia::inferenceserver::StatDuration(*from.success_);
  } else {
    success_ = nullptr;
  }
  // @@protoc_insertion_point(copy_constructor:nvidia.inferenceserver.ProfileRequestStats)
}

inline void ProfileRequestStats::SharedCtor() {
success_ = nullptr;
}

ProfileRequestStats::~ProfileRequestStats() {
  // @@protoc_insertion_point(destructor:nvidia.inferenceserver.ProfileRequestStats)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void ProfileRequestStats::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete success_;
}

void ProfileRequestStats::ArenaDtor(void* object) {
  ProfileRequestStats* _this = reinterpret_cast< ProfileRequestStats* >(object);
  (void)_this;
}
void ProfileRequestStats::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena*) {
}
void ProfileRequestStats::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void ProfileRequestStats::Clear() {
// @@protoc_insertion_point(message_clear_start:nvidia.inferenceserver.ProfileRequestStats)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaForAllocation() == nullptr && success_ != nullptr) {
    delete success_;
  }
  success_ = nullptr;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* ProfileRequestStats::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .nvidia.inferenceserver.StatDuration success = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr = ctx->ParseMessage(_internal_mutable_success(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* ProfileRequestStats::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:nvidia.inferenceserver.ProfileRequestStats)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .nvidia.inferenceserver.StatDuration success = 1;
  if (this->_internal_has_success()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        1, _Internal::success(this), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:nvidia.inferenceserver.ProfileRequestStats)
  return target;
}

size_t ProfileRequestStats::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:nvidia.inferenceserver.ProfileRequestStats)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .nvidia.inferenceserver.StatDuration success = 1;
  if (this->_internal_has_success()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *success_);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData ProfileRequestStats::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    ProfileRequestStats::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*ProfileRequestStats::GetClassData() const { return &_class_data_; }

void ProfileRequestStats::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<ProfileRequestStats *>(to)->MergeFrom(
      static_cast<const ProfileRequestStats &>(from));
}


void ProfileRequestStats::MergeFrom(const ProfileRequestStats& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:nvidia.inferenceserver.ProfileRequestStats)
  GOOGLE_DCHECK_NE(&from, this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_has_success()) {
    _internal_mutable_success()->::nvidia::inferenceserver::StatDuration::MergeFrom(from._internal_success());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void ProfileRequestStats::CopyFrom(const ProfileRequestStats& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:nvidia.inferenceserver.ProfileRequestStats)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ProfileRequestStats::IsInitialized() const {
  return true;
}

void ProfileRequestStats::InternalSwap(ProfileRequestStats* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  swap(success_, other->success_);
}

::PROTOBUF_NAMESPACE_ID::Metadata ProfileRequestStats::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[2]);
}

// ===================================================================

class HealthRequestStats::_Internal {
 public:
  static const ::nvidia::inferenceserver::StatDuration& success(const HealthRequestStats* msg);
};

const ::nvidia::inferenceserver::StatDuration&
HealthRequestStats::_Internal::success(const HealthRequestStats* msg) {
  return *msg->success_;
}
HealthRequestStats::HealthRequestStats(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:nvidia.inferenceserver.HealthRequestStats)
}
HealthRequestStats::HealthRequestStats(const HealthRequestStats& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_success()) {
    success_ = new ::nvidia::inferenceserver::StatDuration(*from.success_);
  } else {
    success_ = nullptr;
  }
  // @@protoc_insertion_point(copy_constructor:nvidia.inferenceserver.HealthRequestStats)
}

inline void HealthRequestStats::SharedCtor() {
success_ = nullptr;
}

HealthRequestStats::~HealthRequestStats() {
  // @@protoc_insertion_point(destructor:nvidia.inferenceserver.HealthRequestStats)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void HealthRequestStats::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete success_;
}

void HealthRequestStats::ArenaDtor(void* object) {
  HealthRequestStats* _this = reinterpret_cast< HealthRequestStats* >(object);
  (void)_this;
}
void HealthRequestStats::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena*) {
}
void HealthRequestStats::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void HealthRequestStats::Clear() {
// @@protoc_insertion_point(message_clear_start:nvidia.inferenceserver.HealthRequestStats)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaForAllocation() == nullptr && success_ != nullptr) {
    delete success_;
  }
  success_ = nullptr;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* HealthRequestStats::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .nvidia.inferenceserver.StatDuration success = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr = ctx->ParseMessage(_internal_mutable_success(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* HealthRequestStats::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:nvidia.inferenceserver.HealthRequestStats)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .nvidia.inferenceserver.StatDuration success = 1;
  if (this->_internal_has_success()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        1, _Internal::success(this), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:nvidia.inferenceserver.HealthRequestStats)
  return target;
}

size_t HealthRequestStats::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:nvidia.inferenceserver.HealthRequestStats)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .nvidia.inferenceserver.StatDuration success = 1;
  if (this->_internal_has_success()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *success_);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData HealthRequestStats::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    HealthRequestStats::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*HealthRequestStats::GetClassData() const { return &_class_data_; }

void HealthRequestStats::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<HealthRequestStats *>(to)->MergeFrom(
      static_cast<const HealthRequestStats &>(from));
}


void HealthRequestStats::MergeFrom(const HealthRequestStats& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:nvidia.inferenceserver.HealthRequestStats)
  GOOGLE_DCHECK_NE(&from, this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_has_success()) {
    _internal_mutable_success()->::nvidia::inferenceserver::StatDuration::MergeFrom(from._internal_success());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void HealthRequestStats::CopyFrom(const HealthRequestStats& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:nvidia.inferenceserver.HealthRequestStats)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool HealthRequestStats::IsInitialized() const {
  return true;
}

void HealthRequestStats::InternalSwap(HealthRequestStats* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  swap(success_, other->success_);
}

::PROTOBUF_NAMESPACE_ID::Metadata HealthRequestStats::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[3]);
}

// ===================================================================

class InferRequestStats::_Internal {
 public:
  static const ::nvidia::inferenceserver::StatDuration& success(const InferRequestStats* msg);
  static const ::nvidia::inferenceserver::StatDuration& failed(const InferRequestStats* msg);
  static const ::nvidia::inferenceserver::StatDuration& compute(const InferRequestStats* msg);
  static const ::nvidia::inferenceserver::StatDuration& queue(const InferRequestStats* msg);
};

const ::nvidia::inferenceserver::StatDuration&
InferRequestStats::_Internal::success(const InferRequestStats* msg) {
  return *msg->success_;
}
const ::nvidia::inferenceserver::StatDuration&
InferRequestStats::_Internal::failed(const InferRequestStats* msg) {
  return *msg->failed_;
}
const ::nvidia::inferenceserver::StatDuration&
InferRequestStats::_Internal::compute(const InferRequestStats* msg) {
  return *msg->compute_;
}
const ::nvidia::inferenceserver::StatDuration&
InferRequestStats::_Internal::queue(const InferRequestStats* msg) {
  return *msg->queue_;
}
InferRequestStats::InferRequestStats(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:nvidia.inferenceserver.InferRequestStats)
}
InferRequestStats::InferRequestStats(const InferRequestStats& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_success()) {
    success_ = new ::nvidia::inferenceserver::StatDuration(*from.success_);
  } else {
    success_ = nullptr;
  }
  if (from._internal_has_failed()) {
    failed_ = new ::nvidia::inferenceserver::StatDuration(*from.failed_);
  } else {
    failed_ = nullptr;
  }
  if (from._internal_has_compute()) {
    compute_ = new ::nvidia::inferenceserver::StatDuration(*from.compute_);
  } else {
    compute_ = nullptr;
  }
  if (from._internal_has_queue()) {
    queue_ = new ::nvidia::inferenceserver::StatDuration(*from.queue_);
  } else {
    queue_ = nullptr;
  }
  // @@protoc_insertion_point(copy_constructor:nvidia.inferenceserver.InferRequestStats)
}

inline void InferRequestStats::SharedCtor() {
::memset(reinterpret_cast<char*>(this) + static_cast<size_t>(
    reinterpret_cast<char*>(&success_) - reinterpret_cast<char*>(this)),
    0, static_cast<size_t>(reinterpret_cast<char*>(&queue_) -
    reinterpret_cast<char*>(&success_)) + sizeof(queue_));
}

InferRequestStats::~InferRequestStats() {
  // @@protoc_insertion_point(destructor:nvidia.inferenceserver.InferRequestStats)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void InferRequestStats::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete success_;
  if (this != internal_default_instance()) delete failed_;
  if (this != internal_default_instance()) delete compute_;
  if (this != internal_default_instance()) delete queue_;
}

void InferRequestStats::ArenaDtor(void* object) {
  InferRequestStats* _this = reinterpret_cast< InferRequestStats* >(object);
  (void)_this;
}
void InferRequestStats::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena*) {
}
void InferRequestStats::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void InferRequestStats::Clear() {
// @@protoc_insertion_point(message_clear_start:nvidia.inferenceserver.InferRequestStats)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaForAllocation() == nullptr && success_ != nullptr) {
    delete success_;
  }
  success_ = nullptr;
  if (GetArenaForAllocation() == nullptr && failed_ != nullptr) {
    delete failed_;
  }
  failed_ = nullptr;
  if (GetArenaForAllocation() == nullptr && compute_ != nullptr) {
    delete compute_;
  }
  compute_ = nullptr;
  if (GetArenaForAllocation() == nullptr && queue_ != nullptr) {
    delete queue_;
  }
  queue_ = nullptr;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* InferRequestStats::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .nvidia.inferenceserver.StatDuration success = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr = ctx->ParseMessage(_internal_mutable_success(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .nvidia.inferenceserver.StatDuration failed = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr = ctx->ParseMessage(_internal_mutable_failed(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .nvidia.inferenceserver.StatDuration compute = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr = ctx->ParseMessage(_internal_mutable_compute(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .nvidia.inferenceserver.StatDuration queue = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 34)) {
          ptr = ctx->ParseMessage(_internal_mutable_queue(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* InferRequestStats::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:nvidia.inferenceserver.InferRequestStats)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .nvidia.inferenceserver.StatDuration success = 1;
  if (this->_internal_has_success()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        1, _Internal::success(this), target, stream);
  }

  // .nvidia.inferenceserver.StatDuration failed = 2;
  if (this->_internal_has_failed()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        2, _Internal::failed(this), target, stream);
  }

  // .nvidia.inferenceserver.StatDuration compute = 3;
  if (this->_internal_has_compute()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        3, _Internal::compute(this), target, stream);
  }

  // .nvidia.inferenceserver.StatDuration queue = 4;
  if (this->_internal_has_queue()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        4, _Internal::queue(this), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:nvidia.inferenceserver.InferRequestStats)
  return target;
}

size_t InferRequestStats::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:nvidia.inferenceserver.InferRequestStats)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .nvidia.inferenceserver.StatDuration success = 1;
  if (this->_internal_has_success()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *success_);
  }

  // .nvidia.inferenceserver.StatDuration failed = 2;
  if (this->_internal_has_failed()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *failed_);
  }

  // .nvidia.inferenceserver.StatDuration compute = 3;
  if (this->_internal_has_compute()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *compute_);
  }

  // .nvidia.inferenceserver.StatDuration queue = 4;
  if (this->_internal_has_queue()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *queue_);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData InferRequestStats::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    InferRequestStats::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*InferRequestStats::GetClassData() const { return &_class_data_; }

void InferRequestStats::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<InferRequestStats *>(to)->MergeFrom(
      static_cast<const InferRequestStats &>(from));
}


void InferRequestStats::MergeFrom(const InferRequestStats& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:nvidia.inferenceserver.InferRequestStats)
  GOOGLE_DCHECK_NE(&from, this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_has_success()) {
    _internal_mutable_success()->::nvidia::inferenceserver::StatDuration::MergeFrom(from._internal_success());
  }
  if (from._internal_has_failed()) {
    _internal_mutable_failed()->::nvidia::inferenceserver::StatDuration::MergeFrom(from._internal_failed());
  }
  if (from._internal_has_compute()) {
    _internal_mutable_compute()->::nvidia::inferenceserver::StatDuration::MergeFrom(from._internal_compute());
  }
  if (from._internal_has_queue()) {
    _internal_mutable_queue()->::nvidia::inferenceserver::StatDuration::MergeFrom(from._internal_queue());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void InferRequestStats::CopyFrom(const InferRequestStats& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:nvidia.inferenceserver.InferRequestStats)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InferRequestStats::IsInitialized() const {
  return true;
}

void InferRequestStats::InternalSwap(InferRequestStats* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(InferRequestStats, queue_)
      + sizeof(InferRequestStats::queue_)
      - PROTOBUF_FIELD_OFFSET(InferRequestStats, success_)>(
          reinterpret_cast<char*>(&success_),
          reinterpret_cast<char*>(&other->success_));
}

::PROTOBUF_NAMESPACE_ID::Metadata InferRequestStats::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[4]);
}

// ===================================================================

ModelVersionStatus_InferStatsEntry_DoNotUse::ModelVersionStatus_InferStatsEntry_DoNotUse() {}
ModelVersionStatus_InferStatsEntry_DoNotUse::ModelVersionStatus_InferStatsEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void ModelVersionStatus_InferStatsEntry_DoNotUse::MergeFrom(const ModelVersionStatus_InferStatsEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata ModelVersionStatus_InferStatsEntry_DoNotUse::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[5]);
}

// ===================================================================

class ModelVersionStatus::_Internal {
 public:
};

ModelVersionStatus::ModelVersionStatus(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned),
  infer_stats_(arena) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:nvidia.inferenceserver.ModelVersionStatus)
}
ModelVersionStatus::ModelVersionStatus(const ModelVersionStatus& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  infer_stats_.MergeFrom(from.infer_stats_);
  ::memcpy(&model_execution_count_, &from.model_execution_count_,
    static_cast<size_t>(reinterpret_cast<char*>(&ready_state_) -
    reinterpret_cast<char*>(&model_execution_count_)) + sizeof(ready_state_));
  // @@protoc_insertion_point(copy_constructor:nvidia.inferenceserver.ModelVersionStatus)
}

inline void ModelVersionStatus::SharedCtor() {
::memset(reinterpret_cast<char*>(this) + static_cast<size_t>(
    reinterpret_cast<char*>(&model_execution_count_) - reinterpret_cast<char*>(this)),
    0, static_cast<size_t>(reinterpret_cast<char*>(&ready_state_) -
    reinterpret_cast<char*>(&model_execution_count_)) + sizeof(ready_state_));
}

ModelVersionStatus::~ModelVersionStatus() {
  // @@protoc_insertion_point(destructor:nvidia.inferenceserver.ModelVersionStatus)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void ModelVersionStatus::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
}

void ModelVersionStatus::ArenaDtor(void* object) {
  ModelVersionStatus* _this = reinterpret_cast< ModelVersionStatus* >(object);
  (void)_this;
  _this->infer_stats_. ~MapField();
}
inline void ModelVersionStatus::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena) {
  if (arena != nullptr) {
    arena->OwnCustomDestructor(this, &ModelVersionStatus::ArenaDtor);
  }
}
void ModelVersionStatus::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void ModelVersionStatus::Clear() {
// @@protoc_insertion_point(message_clear_start:nvidia.inferenceserver.ModelVersionStatus)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  infer_stats_.Clear();
  ::memset(&model_execution_count_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&ready_state_) -
      reinterpret_cast<char*>(&model_execution_count_)) + sizeof(ready_state_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* ModelVersionStatus::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .nvidia.inferenceserver.ModelReadyState ready_state = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 8)) {
          uint64_t val = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
          _internal_set_ready_state(static_cast<::nvidia::inferenceserver::ModelReadyState>(val));
        } else
          goto handle_unusual;
        continue;
      // map<uint32, .nvidia.inferenceserver.InferRequestStats> infer_stats = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(&infer_stats_, ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<18>(ptr));
        } else
          goto handle_unusual;
        continue;
      // uint64 model_execution_count = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          model_execution_count_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // uint64 model_inference_count = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 32)) {
          model_inference_count_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* ModelVersionStatus::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:nvidia.inferenceserver.ModelVersionStatus)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .nvidia.inferenceserver.ModelReadyState ready_state = 1;
  if (this->_internal_ready_state() != 0) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::WriteEnumToArray(
      1, this->_internal_ready_state(), target);
  }

  // map<uint32, .nvidia.inferenceserver.InferRequestStats> infer_stats = 2;
  if (!this->_internal_infer_stats().empty()) {
    typedef ::PROTOBUF_NAMESPACE_ID::Map< uint32_t, ::nvidia::inferenceserver::InferRequestStats >::const_pointer
        ConstPtr;
    typedef ::PROTOBUF_NAMESPACE_ID::internal::SortItem< uint32_t, ConstPtr > SortItem;
    typedef ::PROTOBUF_NAMESPACE_ID::internal::CompareByFirstField<SortItem> Less;

    if (stream->IsSerializationDeterministic() &&
        this->_internal_infer_stats().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->_internal_infer_stats().size()]);
      typedef ::PROTOBUF_NAMESPACE_ID::Map< uint32_t, ::nvidia::inferenceserver::InferRequestStats >::size_type size_type;
      size_type n = 0;
      for (::PROTOBUF_NAMESPACE_ID::Map< uint32_t, ::nvidia::inferenceserver::InferRequestStats >::const_iterator
          it = this->_internal_infer_stats().begin();
          it != this->_internal_infer_stats().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      for (size_type i = 0; i < n; i++) {
        target = ModelVersionStatus_InferStatsEntry_DoNotUse::Funcs::InternalSerialize(2, items[static_cast<ptrdiff_t>(i)].second->first, items[static_cast<ptrdiff_t>(i)].second->second, target, stream);
      }
    } else {
      for (::PROTOBUF_NAMESPACE_ID::Map< uint32_t, ::nvidia::inferenceserver::InferRequestStats >::const_iterator
          it = this->_internal_infer_stats().begin();
          it != this->_internal_infer_stats().end(); ++it) {
        target = ModelVersionStatus_InferStatsEntry_DoNotUse::Funcs::InternalSerialize(2, it->first, it->second, target, stream);
      }
    }
  }

  // uint64 model_execution_count = 3;
  if (this->_internal_model_execution_count() != 0) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::WriteUInt64ToArray(3, this->_internal_model_execution_count(), target);
  }

  // uint64 model_inference_count = 4;
  if (this->_internal_model_inference_count() != 0) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::WriteUInt64ToArray(4, this->_internal_model_inference_count(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:nvidia.inferenceserver.ModelVersionStatus)
  return target;
}

size_t ModelVersionStatus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:nvidia.inferenceserver.ModelVersionStatus)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // map<uint32, .nvidia.inferenceserver.InferRequestStats> infer_stats = 2;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(this->_internal_infer_stats_size());
  for (::PROTOBUF_NAMESPACE_ID::Map< uint32_t, ::nvidia::inferenceserver::InferRequestStats >::const_iterator
      it = this->_internal_infer_stats().begin();
      it != this->_internal_infer_stats().end(); ++it) {
    total_size += ModelVersionStatus_InferStatsEntry_DoNotUse::Funcs::ByteSizeLong(it->first, it->second);
  }

  // uint64 model_execution_count = 3;
  if (this->_internal_model_execution_count() != 0) {
    total_size += ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::UInt64SizePlusOne(this->_internal_model_execution_count());
  }

  // uint64 model_inference_count = 4;
  if (this->_internal_model_inference_count() != 0) {
    total_size += ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::UInt64SizePlusOne(this->_internal_model_inference_count());
  }

  // .nvidia.inferenceserver.ModelReadyState ready_state = 1;
  if (this->_internal_ready_state() != 0) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::EnumSize(this->_internal_ready_state());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData ModelVersionStatus::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    ModelVersionStatus::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*ModelVersionStatus::GetClassData() const { return &_class_data_; }

void ModelVersionStatus::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<ModelVersionStatus *>(to)->MergeFrom(
      static_cast<const ModelVersionStatus &>(from));
}


void ModelVersionStatus::MergeFrom(const ModelVersionStatus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:nvidia.inferenceserver.ModelVersionStatus)
  GOOGLE_DCHECK_NE(&from, this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  infer_stats_.MergeFrom(from.infer_stats_);
  if (from._internal_model_execution_count() != 0) {
    _internal_set_model_execution_count(from._internal_model_execution_count());
  }
  if (from._internal_model_inference_count() != 0) {
    _internal_set_model_inference_count(from._internal_model_inference_count());
  }
  if (from._internal_ready_state() != 0) {
    _internal_set_ready_state(from._internal_ready_state());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void ModelVersionStatus::CopyFrom(const ModelVersionStatus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:nvidia.inferenceserver.ModelVersionStatus)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ModelVersionStatus::IsInitialized() const {
  return true;
}

void ModelVersionStatus::InternalSwap(ModelVersionStatus* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  infer_stats_.InternalSwap(&other->infer_stats_);
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(ModelVersionStatus, ready_state_)
      + sizeof(ModelVersionStatus::ready_state_)
      - PROTOBUF_FIELD_OFFSET(ModelVersionStatus, model_execution_count_)>(
          reinterpret_cast<char*>(&model_execution_count_),
          reinterpret_cast<char*>(&other->model_execution_count_));
}

::PROTOBUF_NAMESPACE_ID::Metadata ModelVersionStatus::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[6]);
}

// ===================================================================

ModelStatus_VersionStatusEntry_DoNotUse::ModelStatus_VersionStatusEntry_DoNotUse() {}
ModelStatus_VersionStatusEntry_DoNotUse::ModelStatus_VersionStatusEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void ModelStatus_VersionStatusEntry_DoNotUse::MergeFrom(const ModelStatus_VersionStatusEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata ModelStatus_VersionStatusEntry_DoNotUse::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[7]);
}

// ===================================================================

class ModelStatus::_Internal {
 public:
  static const ::nvidia::inferenceserver::ModelConfig& config(const ModelStatus* msg);
};

const ::nvidia::inferenceserver::ModelConfig&
ModelStatus::_Internal::config(const ModelStatus* msg) {
  return *msg->config_;
}
void ModelStatus::clear_config() {
  if (GetArenaForAllocation() == nullptr && config_ != nullptr) {
    delete config_;
  }
  config_ = nullptr;
}
ModelStatus::ModelStatus(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned),
  version_status_(arena) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:nvidia.inferenceserver.ModelStatus)
}
ModelStatus::ModelStatus(const ModelStatus& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  version_status_.MergeFrom(from.version_status_);
  if (from._internal_has_config()) {
    config_ = new ::nvidia::inferenceserver::ModelConfig(*from.config_);
  } else {
    config_ = nullptr;
  }
  // @@protoc_insertion_point(copy_constructor:nvidia.inferenceserver.ModelStatus)
}

inline void ModelStatus::SharedCtor() {
config_ = nullptr;
}

ModelStatus::~ModelStatus() {
  // @@protoc_insertion_point(destructor:nvidia.inferenceserver.ModelStatus)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void ModelStatus::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete config_;
}

void ModelStatus::ArenaDtor(void* object) {
  ModelStatus* _this = reinterpret_cast< ModelStatus* >(object);
  (void)_this;
  _this->version_status_. ~MapField();
}
inline void ModelStatus::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena) {
  if (arena != nullptr) {
    arena->OwnCustomDestructor(this, &ModelStatus::ArenaDtor);
  }
}
void ModelStatus::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void ModelStatus::Clear() {
// @@protoc_insertion_point(message_clear_start:nvidia.inferenceserver.ModelStatus)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  version_status_.Clear();
  if (GetArenaForAllocation() == nullptr && config_ != nullptr) {
    delete config_;
  }
  config_ = nullptr;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* ModelStatus::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .nvidia.inferenceserver.ModelConfig config = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr = ctx->ParseMessage(_internal_mutable_config(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // map<int64, .nvidia.inferenceserver.ModelVersionStatus> version_status = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(&version_status_, ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<18>(ptr));
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* ModelStatus::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:nvidia.inferenceserver.ModelStatus)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .nvidia.inferenceserver.ModelConfig config = 1;
  if (this->_internal_has_config()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        1, _Internal::config(this), target, stream);
  }

  // map<int64, .nvidia.inferenceserver.ModelVersionStatus> version_status = 2;
  if (!this->_internal_version_status().empty()) {
    typedef ::PROTOBUF_NAMESPACE_ID::Map< int64_t, ::nvidia::inferenceserver::ModelVersionStatus >::const_pointer
        ConstPtr;
    typedef ::PROTOBUF_NAMESPACE_ID::internal::SortItem< int64_t, ConstPtr > SortItem;
    typedef ::PROTOBUF_NAMESPACE_ID::internal::CompareByFirstField<SortItem> Less;

    if (stream->IsSerializationDeterministic() &&
        this->_internal_version_status().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->_internal_version_status().size()]);
      typedef ::PROTOBUF_NAMESPACE_ID::Map< int64_t, ::nvidia::inferenceserver::ModelVersionStatus >::size_type size_type;
      size_type n = 0;
      for (::PROTOBUF_NAMESPACE_ID::Map< int64_t, ::nvidia::inferenceserver::ModelVersionStatus >::const_iterator
          it = this->_internal_version_status().begin();
          it != this->_internal_version_status().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      for (size_type i = 0; i < n; i++) {
        target = ModelStatus_VersionStatusEntry_DoNotUse::Funcs::InternalSerialize(2, items[static_cast<ptrdiff_t>(i)].second->first, items[static_cast<ptrdiff_t>(i)].second->second, target, stream);
      }
    } else {
      for (::PROTOBUF_NAMESPACE_ID::Map< int64_t, ::nvidia::inferenceserver::ModelVersionStatus >::const_iterator
          it = this->_internal_version_status().begin();
          it != this->_internal_version_status().end(); ++it) {
        target = ModelStatus_VersionStatusEntry_DoNotUse::Funcs::InternalSerialize(2, it->first, it->second, target, stream);
      }
    }
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:nvidia.inferenceserver.ModelStatus)
  return target;
}

size_t ModelStatus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:nvidia.inferenceserver.ModelStatus)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // map<int64, .nvidia.inferenceserver.ModelVersionStatus> version_status = 2;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(this->_internal_version_status_size());
  for (::PROTOBUF_NAMESPACE_ID::Map< int64_t, ::nvidia::inferenceserver::ModelVersionStatus >::const_iterator
      it = this->_internal_version_status().begin();
      it != this->_internal_version_status().end(); ++it) {
    total_size += ModelStatus_VersionStatusEntry_DoNotUse::Funcs::ByteSizeLong(it->first, it->second);
  }

  // .nvidia.inferenceserver.ModelConfig config = 1;
  if (this->_internal_has_config()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *config_);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData ModelStatus::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    ModelStatus::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*ModelStatus::GetClassData() const { return &_class_data_; }

void ModelStatus::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<ModelStatus *>(to)->MergeFrom(
      static_cast<const ModelStatus &>(from));
}


void ModelStatus::MergeFrom(const ModelStatus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:nvidia.inferenceserver.ModelStatus)
  GOOGLE_DCHECK_NE(&from, this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  version_status_.MergeFrom(from.version_status_);
  if (from._internal_has_config()) {
    _internal_mutable_config()->::nvidia::inferenceserver::ModelConfig::MergeFrom(from._internal_config());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void ModelStatus::CopyFrom(const ModelStatus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:nvidia.inferenceserver.ModelStatus)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ModelStatus::IsInitialized() const {
  return true;
}

void ModelStatus::InternalSwap(ModelStatus* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  version_status_.InternalSwap(&other->version_status_);
  swap(config_, other->config_);
}

::PROTOBUF_NAMESPACE_ID::Metadata ModelStatus::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[8]);
}

// ===================================================================

ServerStatus_ModelStatusEntry_DoNotUse::ServerStatus_ModelStatusEntry_DoNotUse() {}
ServerStatus_ModelStatusEntry_DoNotUse::ServerStatus_ModelStatusEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena)
    : SuperType(arena) {}
void ServerStatus_ModelStatusEntry_DoNotUse::MergeFrom(const ServerStatus_ModelStatusEntry_DoNotUse& other) {
  MergeFromInternal(other);
}
::PROTOBUF_NAMESPACE_ID::Metadata ServerStatus_ModelStatusEntry_DoNotUse::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[9]);
}

// ===================================================================

class ServerStatus::_Internal {
 public:
  static const ::nvidia::inferenceserver::StatusRequestStats& status_stats(const ServerStatus* msg);
  static const ::nvidia::inferenceserver::ProfileRequestStats& profile_stats(const ServerStatus* msg);
  static const ::nvidia::inferenceserver::HealthRequestStats& health_stats(const ServerStatus* msg);
};

const ::nvidia::inferenceserver::StatusRequestStats&
ServerStatus::_Internal::status_stats(const ServerStatus* msg) {
  return *msg->status_stats_;
}
const ::nvidia::inferenceserver::ProfileRequestStats&
ServerStatus::_Internal::profile_stats(const ServerStatus* msg) {
  return *msg->profile_stats_;
}
const ::nvidia::inferenceserver::HealthRequestStats&
ServerStatus::_Internal::health_stats(const ServerStatus* msg) {
  return *msg->health_stats_;
}
ServerStatus::ServerStatus(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned),
  model_status_(arena) {
  SharedCtor();
  if (!is_message_owned) {
    RegisterArenaDtor(arena);
  }
  // @@protoc_insertion_point(arena_constructor:nvidia.inferenceserver.ServerStatus)
}
ServerStatus::ServerStatus(const ServerStatus& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  model_status_.MergeFrom(from.model_status_);
  id_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    id_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_id().empty()) {
    id_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, from._internal_id(), 
      GetArenaForAllocation());
  }
  version_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    version_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_version().empty()) {
    version_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, from._internal_version(), 
      GetArenaForAllocation());
  }
  if (from._internal_has_status_stats()) {
    status_stats_ = new ::nvidia::inferenceserver::StatusRequestStats(*from.status_stats_);
  } else {
    status_stats_ = nullptr;
  }
  if (from._internal_has_profile_stats()) {
    profile_stats_ = new ::nvidia::inferenceserver::ProfileRequestStats(*from.profile_stats_);
  } else {
    profile_stats_ = nullptr;
  }
  if (from._internal_has_health_stats()) {
    health_stats_ = new ::nvidia::inferenceserver::HealthRequestStats(*from.health_stats_);
  } else {
    health_stats_ = nullptr;
  }
  ::memcpy(&uptime_ns_, &from.uptime_ns_,
    static_cast<size_t>(reinterpret_cast<char*>(&ready_state_) -
    reinterpret_cast<char*>(&uptime_ns_)) + sizeof(ready_state_));
  // @@protoc_insertion_point(copy_constructor:nvidia.inferenceserver.ServerStatus)
}

inline void ServerStatus::SharedCtor() {
id_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  id_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
version_.UnsafeSetDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  version_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
::memset(reinterpret_cast<char*>(this) + static_cast<size_t>(
    reinterpret_cast<char*>(&status_stats_) - reinterpret_cast<char*>(this)),
    0, static_cast<size_t>(reinterpret_cast<char*>(&ready_state_) -
    reinterpret_cast<char*>(&status_stats_)) + sizeof(ready_state_));
}

ServerStatus::~ServerStatus() {
  // @@protoc_insertion_point(destructor:nvidia.inferenceserver.ServerStatus)
  if (GetArenaForAllocation() != nullptr) return;
  SharedDtor();
  _internal_metadata_.Delete<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

inline void ServerStatus::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  id_.DestroyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  version_.DestroyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete status_stats_;
  if (this != internal_default_instance()) delete profile_stats_;
  if (this != internal_default_instance()) delete health_stats_;
}

void ServerStatus::ArenaDtor(void* object) {
  ServerStatus* _this = reinterpret_cast< ServerStatus* >(object);
  (void)_this;
  _this->model_status_. ~MapField();
}
inline void ServerStatus::RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena) {
  if (arena != nullptr) {
    arena->OwnCustomDestructor(this, &ServerStatus::ArenaDtor);
  }
}
void ServerStatus::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}

void ServerStatus::Clear() {
// @@protoc_insertion_point(message_clear_start:nvidia.inferenceserver.ServerStatus)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  model_status_.Clear();
  id_.ClearToEmpty();
  version_.ClearToEmpty();
  if (GetArenaForAllocation() == nullptr && status_stats_ != nullptr) {
    delete status_stats_;
  }
  status_stats_ = nullptr;
  if (GetArenaForAllocation() == nullptr && profile_stats_ != nullptr) {
    delete profile_stats_;
  }
  profile_stats_ = nullptr;
  if (GetArenaForAllocation() == nullptr && health_stats_ != nullptr) {
    delete health_stats_;
  }
  health_stats_ = nullptr;
  ::memset(&uptime_ns_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&ready_state_) -
      reinterpret_cast<char*>(&uptime_ns_)) + sizeof(ready_state_));
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* ServerStatus::_InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::PROTOBUF_NAMESPACE_ID::internal::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // string id = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          auto str = _internal_mutable_id();
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(::PROTOBUF_NAMESPACE_ID::internal::VerifyUTF8(str, "nvidia.inferenceserver.ServerStatus.id"));
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // string version = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          auto str = _internal_mutable_version();
          ptr = ::PROTOBUF_NAMESPACE_ID::internal::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(::PROTOBUF_NAMESPACE_ID::internal::VerifyUTF8(str, "nvidia.inferenceserver.ServerStatus.version"));
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // uint64 uptime_ns = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 24)) {
          uptime_ns_ = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // map<string, .nvidia.inferenceserver.ModelStatus> model_status = 4;
      case 4:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 34)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(&model_status_, ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<34>(ptr));
        } else
          goto handle_unusual;
        continue;
      // .nvidia.inferenceserver.StatusRequestStats status_stats = 5;
      case 5:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 42)) {
          ptr = ctx->ParseMessage(_internal_mutable_status_stats(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;
      case 6:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 50)) {
          ptr = ctx->ParseMessage(_internal_mutable_profile_stats(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .nvidia.inferenceserver.ServerReadyState ready_state = 7;
      case 7:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 56)) {
          uint64_t val = ::PROTOBUF_NAMESPACE_ID::internal::ReadVarint64(&ptr);
          CHK_(ptr);
          _internal_set_ready_state(static_cast<::nvidia::inferenceserver::ServerReadyState>(val));
        } else
          goto handle_unusual;
        continue;
      // .nvidia.inferenceserver.HealthRequestStats health_stats = 8;
      case 8:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 66)) {
          ptr = ctx->ParseMessage(_internal_mutable_health_stats(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* ServerStatus::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:nvidia.inferenceserver.ServerStatus)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // string id = 1;
  if (!this->_internal_id().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_id().data(), static_cast<int>(this->_internal_id().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "nvidia.inferenceserver.ServerStatus.id");
    target = stream->WriteStringMaybeAliased(
        1, this->_internal_id(), target);
  }

  // string version = 2;
  if (!this->_internal_version().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_version().data(), static_cast<int>(this->_internal_version().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "nvidia.inferenceserver.ServerStatus.version");
    target = stream->WriteStringMaybeAliased(
        2, this->_internal_version(), target);
  }

  // uint64 uptime_ns = 3;
  if (this->_internal_uptime_ns() != 0) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::WriteUInt64ToArray(3, this->_internal_uptime_ns(), target);
  }

  // map<string, .nvidia.inferenceserver.ModelStatus> model_status = 4;
  if (!this->_internal_model_status().empty()) {
    typedef ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::nvidia::inferenceserver::ModelStatus >::const_pointer
        ConstPtr;
    typedef ConstPtr SortItem;
    typedef ::PROTOBUF_NAMESPACE_ID::internal::CompareByDerefFirst<SortItem> Less;
    struct Utf8Check {
      static void Check(ConstPtr p) {
        (void)p;
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
          p->first.data(), static_cast<int>(p->first.length()),
          ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
          "nvidia.inferenceserver.ServerStatus.ModelStatusEntry.key");
      }
    };

    if (stream->IsSerializationDeterministic() &&
        this->_internal_model_status().size() > 1) {
      ::std::unique_ptr<SortItem[]> items(
          new SortItem[this->_internal_model_status().size()]);
      typedef ::PROTOBUF_NAMESPACE_ID::Map< std::string, ::nvidia::inferenceserver::ModelStatus >::size_type size_type;
      size_type n = 0;
      for (::PROTOBUF_NAMESPACE_ID::Map< std::string, ::nvidia::inferenceserver::ModelStatus >::const_iterator
          it = this->_internal_model_status().begin();
          it != this->_internal_model_status().end(); ++it, ++n) {
        items[static_cast<ptrdiff_t>(n)] = SortItem(&*it);
      }
      ::std::sort(&items[0], &items[static_cast<ptrdiff_t>(n)], Less());
      for (size_type i = 0; i < n; i++) {
        target = ServerStatus_ModelStatusEntry_DoNotUse::Funcs::InternalSerialize(4, items[static_cast<ptrdiff_t>(i)]->first, items[static_cast<ptrdiff_t>(i)]->second, target, stream);
        Utf8Check::Check(&(*items[static_cast<ptrdiff_t>(i)]));
      }
    } else {
      for (::PROTOBUF_NAMESPACE_ID::Map< std::string, ::nvidia::inferenceserver::ModelStatus >::const_iterator
          it = this->_internal_model_status().begin();
          it != this->_internal_model_status().end(); ++it) {
        target = ServerStatus_ModelStatusEntry_DoNotUse::Funcs::InternalSerialize(4, it->first, it->second, target, stream);
        Utf8Check::Check(&(*it));
      }
    }
  }

  // .nvidia.inferenceserver.StatusRequestStats status_stats = 5;
  if (this->_internal_has_status_stats()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        5, _Internal::status_stats(this), target, stream);
  }

  // .nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;
  if (this->_internal_has_profile_stats()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        6, _Internal::profile_stats(this), target, stream);
  }

  // .nvidia.inferenceserver.ServerReadyState ready_state = 7;
  if (this->_internal_ready_state() != 0) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::WriteEnumToArray(
      7, this->_internal_ready_state(), target);
  }

  // .nvidia.inferenceserver.HealthRequestStats health_stats = 8;
  if (this->_internal_has_health_stats()) {
    target = stream->EnsureSpace(target);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(
        8, _Internal::health_stats(this), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:nvidia.inferenceserver.ServerStatus)
  return target;
}

size_t ServerStatus::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:nvidia.inferenceserver.ServerStatus)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // map<string, .nvidia.inferenceserver.ModelStatus> model_status = 4;
  total_size += 1 *
      ::PROTOBUF_NAMESPACE_ID::internal::FromIntSize(this->_internal_model_status_size());
  for (::PROTOBUF_NAMESPACE_ID::Map< std::string, ::nvidia::inferenceserver::ModelStatus >::const_iterator
      it = this->_internal_model_status().begin();
      it != this->_internal_model_status().end(); ++it) {
    total_size += ServerStatus_ModelStatusEntry_DoNotUse::Funcs::ByteSizeLong(it->first, it->second);
  }

  // string id = 1;
  if (!this->_internal_id().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_id());
  }

  // string version = 2;
  if (!this->_internal_version().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_version());
  }

  // .nvidia.inferenceserver.StatusRequestStats status_stats = 5;
  if (this->_internal_has_status_stats()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *status_stats_);
  }

  // .nvidia.inferenceserver.ProfileRequestStats profile_stats = 6;
  if (this->_internal_has_profile_stats()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *profile_stats_);
  }

  // .nvidia.inferenceserver.HealthRequestStats health_stats = 8;
  if (this->_internal_has_health_stats()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *health_stats_);
  }

  // uint64 uptime_ns = 3;
  if (this->_internal_uptime_ns() != 0) {
    total_size += ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::UInt64SizePlusOne(this->_internal_uptime_ns());
  }

  // .nvidia.inferenceserver.ServerReadyState ready_state = 7;
  if (this->_internal_ready_state() != 0) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::EnumSize(this->_internal_ready_state());
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData ServerStatus::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSizeCheck,
    ServerStatus::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*ServerStatus::GetClassData() const { return &_class_data_; }

void ServerStatus::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to,
                      const ::PROTOBUF_NAMESPACE_ID::Message& from) {
  static_cast<ServerStatus *>(to)->MergeFrom(
      static_cast<const ServerStatus &>(from));
}


void ServerStatus::MergeFrom(const ServerStatus& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:nvidia.inferenceserver.ServerStatus)
  GOOGLE_DCHECK_NE(&from, this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  model_status_.MergeFrom(from.model_status_);
  if (!from._internal_id().empty()) {
    _internal_set_id(from._internal_id());
  }
  if (!from._internal_version().empty()) {
    _internal_set_version(from._internal_version());
  }
  if (from._internal_has_status_stats()) {
    _internal_mutable_status_stats()->::nvidia::inferenceserver::StatusRequestStats::MergeFrom(from._internal_status_stats());
  }
  if (from._internal_has_profile_stats()) {
    _internal_mutable_profile_stats()->::nvidia::inferenceserver::ProfileRequestStats::MergeFrom(from._internal_profile_stats());
  }
  if (from._internal_has_health_stats()) {
    _internal_mutable_health_stats()->::nvidia::inferenceserver::HealthRequestStats::MergeFrom(from._internal_health_stats());
  }
  if (from._internal_uptime_ns() != 0) {
    _internal_set_uptime_ns(from._internal_uptime_ns());
  }
  if (from._internal_ready_state() != 0) {
    _internal_set_ready_state(from._internal_ready_state());
  }
  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void ServerStatus::CopyFrom(const ServerStatus& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:nvidia.inferenceserver.ServerStatus)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ServerStatus::IsInitialized() const {
  return true;
}

void ServerStatus::InternalSwap(ServerStatus* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  model_status_.InternalSwap(&other->model_status_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(),
      &id_, lhs_arena,
      &other->id_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(),
      &version_, lhs_arena,
      &other->version_, rhs_arena
  );
  ::PROTOBUF_NAMESPACE_ID::internal::memswap<
      PROTOBUF_FIELD_OFFSET(ServerStatus, ready_state_)
      + sizeof(ServerStatus::ready_state_)
      - PROTOBUF_FIELD_OFFSET(ServerStatus, status_stats_)>(
          reinterpret_cast<char*>(&status_stats_),
          reinterpret_cast<char*>(&other->status_stats_));
}

::PROTOBUF_NAMESPACE_ID::Metadata ServerStatus::GetMetadata() const {
  return ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(
      &descriptor_table_server_5fstatus_2eproto_getter, &descriptor_table_server_5fstatus_2eproto_once,
      file_level_metadata_server_5fstatus_2eproto[10]);
}

// @@protoc_insertion_point(namespace_scope)
}  // namespace inferenceserver
}  // namespace nvidia
PROTOBUF_NAMESPACE_OPEN
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::StatDuration* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::StatDuration >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::StatDuration >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::StatusRequestStats* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::StatusRequestStats >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::StatusRequestStats >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::ProfileRequestStats* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::ProfileRequestStats >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::ProfileRequestStats >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::HealthRequestStats* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::HealthRequestStats >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::HealthRequestStats >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::InferRequestStats* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::InferRequestStats >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::InferRequestStats >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::ModelVersionStatus_InferStatsEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::ModelVersionStatus* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::ModelVersionStatus >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::ModelVersionStatus >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::ModelStatus_VersionStatusEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::ModelStatus* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::ModelStatus >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::ModelStatus >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::ServerStatus_ModelStatusEntry_DoNotUse >(arena);
}
template<> PROTOBUF_NOINLINE ::nvidia::inferenceserver::ServerStatus* Arena::CreateMaybeMessage< ::nvidia::inferenceserver::ServerStatus >(Arena* arena) {
  return Arena::CreateMessageInternal< ::nvidia::inferenceserver::ServerStatus >(arena);
}
PROTOBUF_NAMESPACE_CLOSE

// @@protoc_insertion_point(global_scope)
#include <google/protobuf/port_undef.inc>
