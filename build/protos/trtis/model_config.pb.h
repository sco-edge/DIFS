// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: model_config.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_model_5fconfig_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_model_5fconfig_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3019000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3019004 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_bases.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/map.h>  // IWYU pragma: export
#include <google/protobuf/map_entry.h>
#include <google/protobuf/map_field_inl.h>
#include <google/protobuf/generated_enum_reflection.h>
#include <google/protobuf/unknown_field_set.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_model_5fconfig_2eproto
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct TableStruct_model_5fconfig_2eproto {
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTableField entries[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::AuxiliaryParseTableField aux[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTable schema[14]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::FieldMetadata field_metadata[];
  static const ::PROTOBUF_NAMESPACE_ID::internal::SerializationTable serialization_table[];
  static const uint32_t offsets[];
};
extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_model_5fconfig_2eproto;
namespace nvidia {
namespace inferenceserver {
class ModelConfig;
struct ModelConfigDefaultTypeInternal;
extern ModelConfigDefaultTypeInternal _ModelConfig_default_instance_;
class ModelConfig_CcModelFilenamesEntry_DoNotUse;
struct ModelConfig_CcModelFilenamesEntry_DoNotUseDefaultTypeInternal;
extern ModelConfig_CcModelFilenamesEntry_DoNotUseDefaultTypeInternal _ModelConfig_CcModelFilenamesEntry_DoNotUse_default_instance_;
class ModelConfig_TagsEntry_DoNotUse;
struct ModelConfig_TagsEntry_DoNotUseDefaultTypeInternal;
extern ModelConfig_TagsEntry_DoNotUseDefaultTypeInternal _ModelConfig_TagsEntry_DoNotUse_default_instance_;
class ModelDynamicBatching;
struct ModelDynamicBatchingDefaultTypeInternal;
extern ModelDynamicBatchingDefaultTypeInternal _ModelDynamicBatching_default_instance_;
class ModelInput;
struct ModelInputDefaultTypeInternal;
extern ModelInputDefaultTypeInternal _ModelInput_default_instance_;
class ModelInstanceGroup;
struct ModelInstanceGroupDefaultTypeInternal;
extern ModelInstanceGroupDefaultTypeInternal _ModelInstanceGroup_default_instance_;
class ModelOptimizationPolicy;
struct ModelOptimizationPolicyDefaultTypeInternal;
extern ModelOptimizationPolicyDefaultTypeInternal _ModelOptimizationPolicy_default_instance_;
class ModelOptimizationPolicy_Graph;
struct ModelOptimizationPolicy_GraphDefaultTypeInternal;
extern ModelOptimizationPolicy_GraphDefaultTypeInternal _ModelOptimizationPolicy_Graph_default_instance_;
class ModelOutput;
struct ModelOutputDefaultTypeInternal;
extern ModelOutputDefaultTypeInternal _ModelOutput_default_instance_;
class ModelSequenceBatching;
struct ModelSequenceBatchingDefaultTypeInternal;
extern ModelSequenceBatchingDefaultTypeInternal _ModelSequenceBatching_default_instance_;
class ModelVersionPolicy;
struct ModelVersionPolicyDefaultTypeInternal;
extern ModelVersionPolicyDefaultTypeInternal _ModelVersionPolicy_default_instance_;
class ModelVersionPolicy_All;
struct ModelVersionPolicy_AllDefaultTypeInternal;
extern ModelVersionPolicy_AllDefaultTypeInternal _ModelVersionPolicy_All_default_instance_;
class ModelVersionPolicy_Latest;
struct ModelVersionPolicy_LatestDefaultTypeInternal;
extern ModelVersionPolicy_LatestDefaultTypeInternal _ModelVersionPolicy_Latest_default_instance_;
class ModelVersionPolicy_Specific;
struct ModelVersionPolicy_SpecificDefaultTypeInternal;
extern ModelVersionPolicy_SpecificDefaultTypeInternal _ModelVersionPolicy_Specific_default_instance_;
}  // namespace inferenceserver
}  // namespace nvidia
PROTOBUF_NAMESPACE_OPEN
template<> ::nvidia::inferenceserver::ModelConfig* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelConfig>(Arena*);
template<> ::nvidia::inferenceserver::ModelConfig_CcModelFilenamesEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelConfig_CcModelFilenamesEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelConfig_TagsEntry_DoNotUse* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelConfig_TagsEntry_DoNotUse>(Arena*);
template<> ::nvidia::inferenceserver::ModelDynamicBatching* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelDynamicBatching>(Arena*);
template<> ::nvidia::inferenceserver::ModelInput* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelInput>(Arena*);
template<> ::nvidia::inferenceserver::ModelInstanceGroup* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelInstanceGroup>(Arena*);
template<> ::nvidia::inferenceserver::ModelOptimizationPolicy* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy>(Arena*);
template<> ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_Graph>(Arena*);
template<> ::nvidia::inferenceserver::ModelOutput* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelOutput>(Arena*);
template<> ::nvidia::inferenceserver::ModelSequenceBatching* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelSequenceBatching>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionPolicy* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionPolicy_All* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy_All>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionPolicy_Latest* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy_Latest>(Arena*);
template<> ::nvidia::inferenceserver::ModelVersionPolicy_Specific* Arena::CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy_Specific>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace nvidia {
namespace inferenceserver {

enum ModelInstanceGroup_Kind : int {
  ModelInstanceGroup_Kind_KIND_AUTO = 0,
  ModelInstanceGroup_Kind_KIND_GPU = 1,
  ModelInstanceGroup_Kind_KIND_CPU = 2,
  ModelInstanceGroup_Kind_ModelInstanceGroup_Kind_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  ModelInstanceGroup_Kind_ModelInstanceGroup_Kind_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool ModelInstanceGroup_Kind_IsValid(int value);
constexpr ModelInstanceGroup_Kind ModelInstanceGroup_Kind_Kind_MIN = ModelInstanceGroup_Kind_KIND_AUTO;
constexpr ModelInstanceGroup_Kind ModelInstanceGroup_Kind_Kind_MAX = ModelInstanceGroup_Kind_KIND_CPU;
constexpr int ModelInstanceGroup_Kind_Kind_ARRAYSIZE = ModelInstanceGroup_Kind_Kind_MAX + 1;

const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* ModelInstanceGroup_Kind_descriptor();
template<typename T>
inline const std::string& ModelInstanceGroup_Kind_Name(T enum_t_value) {
  static_assert(::std::is_same<T, ModelInstanceGroup_Kind>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function ModelInstanceGroup_Kind_Name.");
  return ::PROTOBUF_NAMESPACE_ID::internal::NameOfEnum(
    ModelInstanceGroup_Kind_descriptor(), enum_t_value);
}
inline bool ModelInstanceGroup_Kind_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, ModelInstanceGroup_Kind* value) {
  return ::PROTOBUF_NAMESPACE_ID::internal::ParseNamedEnum<ModelInstanceGroup_Kind>(
    ModelInstanceGroup_Kind_descriptor(), name, value);
}
enum ModelInput_Format : int {
  ModelInput_Format_FORMAT_NONE = 0,
  ModelInput_Format_FORMAT_NHWC = 1,
  ModelInput_Format_FORMAT_NCHW = 2,
  ModelInput_Format_ModelInput_Format_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  ModelInput_Format_ModelInput_Format_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool ModelInput_Format_IsValid(int value);
constexpr ModelInput_Format ModelInput_Format_Format_MIN = ModelInput_Format_FORMAT_NONE;
constexpr ModelInput_Format ModelInput_Format_Format_MAX = ModelInput_Format_FORMAT_NCHW;
constexpr int ModelInput_Format_Format_ARRAYSIZE = ModelInput_Format_Format_MAX + 1;

const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* ModelInput_Format_descriptor();
template<typename T>
inline const std::string& ModelInput_Format_Name(T enum_t_value) {
  static_assert(::std::is_same<T, ModelInput_Format>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function ModelInput_Format_Name.");
  return ::PROTOBUF_NAMESPACE_ID::internal::NameOfEnum(
    ModelInput_Format_descriptor(), enum_t_value);
}
inline bool ModelInput_Format_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, ModelInput_Format* value) {
  return ::PROTOBUF_NAMESPACE_ID::internal::ParseNamedEnum<ModelInput_Format>(
    ModelInput_Format_descriptor(), name, value);
}
enum ModelOptimizationPolicy_ModelPriority : int {
  ModelOptimizationPolicy_ModelPriority_PRIORITY_DEFAULT = 0,
  ModelOptimizationPolicy_ModelPriority_PRIORITY_MAX = 1,
  ModelOptimizationPolicy_ModelPriority_PRIORITY_MIN = 2,
  ModelOptimizationPolicy_ModelPriority_ModelOptimizationPolicy_ModelPriority_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  ModelOptimizationPolicy_ModelPriority_ModelOptimizationPolicy_ModelPriority_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool ModelOptimizationPolicy_ModelPriority_IsValid(int value);
constexpr ModelOptimizationPolicy_ModelPriority ModelOptimizationPolicy_ModelPriority_ModelPriority_MIN = ModelOptimizationPolicy_ModelPriority_PRIORITY_DEFAULT;
constexpr ModelOptimizationPolicy_ModelPriority ModelOptimizationPolicy_ModelPriority_ModelPriority_MAX = ModelOptimizationPolicy_ModelPriority_PRIORITY_MIN;
constexpr int ModelOptimizationPolicy_ModelPriority_ModelPriority_ARRAYSIZE = ModelOptimizationPolicy_ModelPriority_ModelPriority_MAX + 1;

const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* ModelOptimizationPolicy_ModelPriority_descriptor();
template<typename T>
inline const std::string& ModelOptimizationPolicy_ModelPriority_Name(T enum_t_value) {
  static_assert(::std::is_same<T, ModelOptimizationPolicy_ModelPriority>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function ModelOptimizationPolicy_ModelPriority_Name.");
  return ::PROTOBUF_NAMESPACE_ID::internal::NameOfEnum(
    ModelOptimizationPolicy_ModelPriority_descriptor(), enum_t_value);
}
inline bool ModelOptimizationPolicy_ModelPriority_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, ModelOptimizationPolicy_ModelPriority* value) {
  return ::PROTOBUF_NAMESPACE_ID::internal::ParseNamedEnum<ModelOptimizationPolicy_ModelPriority>(
    ModelOptimizationPolicy_ModelPriority_descriptor(), name, value);
}
enum DataType : int {
  TYPE_INVALID = 0,
  TYPE_BOOL = 1,
  TYPE_UINT8 = 2,
  TYPE_UINT16 = 3,
  TYPE_UINT32 = 4,
  TYPE_UINT64 = 5,
  TYPE_INT8 = 6,
  TYPE_INT16 = 7,
  TYPE_INT32 = 8,
  TYPE_INT64 = 9,
  TYPE_FP16 = 10,
  TYPE_FP32 = 11,
  TYPE_FP64 = 12,
  TYPE_STRING = 13,
  DataType_INT_MIN_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::min(),
  DataType_INT_MAX_SENTINEL_DO_NOT_USE_ = std::numeric_limits<int32_t>::max()
};
bool DataType_IsValid(int value);
constexpr DataType DataType_MIN = TYPE_INVALID;
constexpr DataType DataType_MAX = TYPE_STRING;
constexpr int DataType_ARRAYSIZE = DataType_MAX + 1;

const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor* DataType_descriptor();
template<typename T>
inline const std::string& DataType_Name(T enum_t_value) {
  static_assert(::std::is_same<T, DataType>::value ||
    ::std::is_integral<T>::value,
    "Incorrect type passed to function DataType_Name.");
  return ::PROTOBUF_NAMESPACE_ID::internal::NameOfEnum(
    DataType_descriptor(), enum_t_value);
}
inline bool DataType_Parse(
    ::PROTOBUF_NAMESPACE_ID::ConstStringParam name, DataType* value) {
  return ::PROTOBUF_NAMESPACE_ID::internal::ParseNamedEnum<DataType>(
    DataType_descriptor(), name, value);
}
// ===================================================================

class ModelInstanceGroup final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelInstanceGroup) */ {
 public:
  inline ModelInstanceGroup() : ModelInstanceGroup(nullptr) {}
  ~ModelInstanceGroup() override;
  explicit constexpr ModelInstanceGroup(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelInstanceGroup(const ModelInstanceGroup& from);
  ModelInstanceGroup(ModelInstanceGroup&& from) noexcept
    : ModelInstanceGroup() {
    *this = ::std::move(from);
  }

  inline ModelInstanceGroup& operator=(const ModelInstanceGroup& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelInstanceGroup& operator=(ModelInstanceGroup&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelInstanceGroup& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelInstanceGroup* internal_default_instance() {
    return reinterpret_cast<const ModelInstanceGroup*>(
               &_ModelInstanceGroup_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(ModelInstanceGroup& a, ModelInstanceGroup& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelInstanceGroup* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelInstanceGroup* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelInstanceGroup* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelInstanceGroup>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelInstanceGroup& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelInstanceGroup& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelInstanceGroup* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelInstanceGroup";
  }
  protected:
  explicit ModelInstanceGroup(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelInstanceGroup_Kind Kind;
  static constexpr Kind KIND_AUTO =
    ModelInstanceGroup_Kind_KIND_AUTO;
  static constexpr Kind KIND_GPU =
    ModelInstanceGroup_Kind_KIND_GPU;
  static constexpr Kind KIND_CPU =
    ModelInstanceGroup_Kind_KIND_CPU;
  static inline bool Kind_IsValid(int value) {
    return ModelInstanceGroup_Kind_IsValid(value);
  }
  static constexpr Kind Kind_MIN =
    ModelInstanceGroup_Kind_Kind_MIN;
  static constexpr Kind Kind_MAX =
    ModelInstanceGroup_Kind_Kind_MAX;
  static constexpr int Kind_ARRAYSIZE =
    ModelInstanceGroup_Kind_Kind_ARRAYSIZE;
  static inline const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor*
  Kind_descriptor() {
    return ModelInstanceGroup_Kind_descriptor();
  }
  template<typename T>
  static inline const std::string& Kind_Name(T enum_t_value) {
    static_assert(::std::is_same<T, Kind>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function Kind_Name.");
    return ModelInstanceGroup_Kind_Name(enum_t_value);
  }
  static inline bool Kind_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      Kind* value) {
    return ModelInstanceGroup_Kind_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kGpusFieldNumber = 3,
    kNameFieldNumber = 1,
    kCountFieldNumber = 2,
    kKindFieldNumber = 4,
  };
  // repeated int32 gpus = 3;
  int gpus_size() const;
  private:
  int _internal_gpus_size() const;
  public:
  void clear_gpus();
  private:
  int32_t _internal_gpus(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >&
      _internal_gpus() const;
  void _internal_add_gpus(int32_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >*
      _internal_mutable_gpus();
  public:
  int32_t gpus(int index) const;
  void set_gpus(int index, int32_t value);
  void add_gpus(int32_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >&
      gpus() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >*
      mutable_gpus();

  // string name = 1;
  void clear_name();
  const std::string& name() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_name(ArgT0&& arg0, ArgT... args);
  std::string* mutable_name();
  PROTOBUF_NODISCARD std::string* release_name();
  void set_allocated_name(std::string* name);
  private:
  const std::string& _internal_name() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_name(const std::string& value);
  std::string* _internal_mutable_name();
  public:

  // int32 count = 2;
  void clear_count();
  int32_t count() const;
  void set_count(int32_t value);
  private:
  int32_t _internal_count() const;
  void _internal_set_count(int32_t value);
  public:

  // .nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;
  void clear_kind();
  ::nvidia::inferenceserver::ModelInstanceGroup_Kind kind() const;
  void set_kind(::nvidia::inferenceserver::ModelInstanceGroup_Kind value);
  private:
  ::nvidia::inferenceserver::ModelInstanceGroup_Kind _internal_kind() const;
  void _internal_set_kind(::nvidia::inferenceserver::ModelInstanceGroup_Kind value);
  public:

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInstanceGroup)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t > gpus_;
  mutable std::atomic<int> _gpus_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr name_;
  int32_t count_;
  int kind_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelInput final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelInput) */ {
 public:
  inline ModelInput() : ModelInput(nullptr) {}
  ~ModelInput() override;
  explicit constexpr ModelInput(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelInput(const ModelInput& from);
  ModelInput(ModelInput&& from) noexcept
    : ModelInput() {
    *this = ::std::move(from);
  }

  inline ModelInput& operator=(const ModelInput& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelInput& operator=(ModelInput&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelInput& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelInput* internal_default_instance() {
    return reinterpret_cast<const ModelInput*>(
               &_ModelInput_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  friend void swap(ModelInput& a, ModelInput& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelInput* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelInput* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelInput* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelInput>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelInput& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelInput& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelInput* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelInput";
  }
  protected:
  explicit ModelInput(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelInput_Format Format;
  static constexpr Format FORMAT_NONE =
    ModelInput_Format_FORMAT_NONE;
  static constexpr Format FORMAT_NHWC =
    ModelInput_Format_FORMAT_NHWC;
  static constexpr Format FORMAT_NCHW =
    ModelInput_Format_FORMAT_NCHW;
  static inline bool Format_IsValid(int value) {
    return ModelInput_Format_IsValid(value);
  }
  static constexpr Format Format_MIN =
    ModelInput_Format_Format_MIN;
  static constexpr Format Format_MAX =
    ModelInput_Format_Format_MAX;
  static constexpr int Format_ARRAYSIZE =
    ModelInput_Format_Format_ARRAYSIZE;
  static inline const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor*
  Format_descriptor() {
    return ModelInput_Format_descriptor();
  }
  template<typename T>
  static inline const std::string& Format_Name(T enum_t_value) {
    static_assert(::std::is_same<T, Format>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function Format_Name.");
    return ModelInput_Format_Name(enum_t_value);
  }
  static inline bool Format_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      Format* value) {
    return ModelInput_Format_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kDimsFieldNumber = 4,
    kNameFieldNumber = 1,
    kDataTypeFieldNumber = 2,
    kFormatFieldNumber = 3,
  };
  // repeated int64 dims = 4;
  int dims_size() const;
  private:
  int _internal_dims_size() const;
  public:
  void clear_dims();
  private:
  int64_t _internal_dims(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_dims() const;
  void _internal_add_dims(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_dims();
  public:
  int64_t dims(int index) const;
  void set_dims(int index, int64_t value);
  void add_dims(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      dims() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_dims();

  // string name = 1;
  void clear_name();
  const std::string& name() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_name(ArgT0&& arg0, ArgT... args);
  std::string* mutable_name();
  PROTOBUF_NODISCARD std::string* release_name();
  void set_allocated_name(std::string* name);
  private:
  const std::string& _internal_name() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_name(const std::string& value);
  std::string* _internal_mutable_name();
  public:

  // .nvidia.inferenceserver.DataType data_type = 2;
  void clear_data_type();
  ::nvidia::inferenceserver::DataType data_type() const;
  void set_data_type(::nvidia::inferenceserver::DataType value);
  private:
  ::nvidia::inferenceserver::DataType _internal_data_type() const;
  void _internal_set_data_type(::nvidia::inferenceserver::DataType value);
  public:

  // .nvidia.inferenceserver.ModelInput.Format format = 3;
  void clear_format();
  ::nvidia::inferenceserver::ModelInput_Format format() const;
  void set_format(::nvidia::inferenceserver::ModelInput_Format value);
  private:
  ::nvidia::inferenceserver::ModelInput_Format _internal_format() const;
  void _internal_set_format(::nvidia::inferenceserver::ModelInput_Format value);
  public:

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInput)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > dims_;
  mutable std::atomic<int> _dims_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr name_;
  int data_type_;
  int format_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelOutput final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOutput) */ {
 public:
  inline ModelOutput() : ModelOutput(nullptr) {}
  ~ModelOutput() override;
  explicit constexpr ModelOutput(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelOutput(const ModelOutput& from);
  ModelOutput(ModelOutput&& from) noexcept
    : ModelOutput() {
    *this = ::std::move(from);
  }

  inline ModelOutput& operator=(const ModelOutput& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelOutput& operator=(ModelOutput&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelOutput& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelOutput* internal_default_instance() {
    return reinterpret_cast<const ModelOutput*>(
               &_ModelOutput_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  friend void swap(ModelOutput& a, ModelOutput& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelOutput* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelOutput* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelOutput* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelOutput>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelOutput& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelOutput& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOutput* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelOutput";
  }
  protected:
  explicit ModelOutput(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kDimsFieldNumber = 3,
    kNameFieldNumber = 1,
    kLabelFilenameFieldNumber = 4,
    kDataTypeFieldNumber = 2,
  };
  // repeated int64 dims = 3;
  int dims_size() const;
  private:
  int _internal_dims_size() const;
  public:
  void clear_dims();
  private:
  int64_t _internal_dims(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_dims() const;
  void _internal_add_dims(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_dims();
  public:
  int64_t dims(int index) const;
  void set_dims(int index, int64_t value);
  void add_dims(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      dims() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_dims();

  // string name = 1;
  void clear_name();
  const std::string& name() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_name(ArgT0&& arg0, ArgT... args);
  std::string* mutable_name();
  PROTOBUF_NODISCARD std::string* release_name();
  void set_allocated_name(std::string* name);
  private:
  const std::string& _internal_name() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_name(const std::string& value);
  std::string* _internal_mutable_name();
  public:

  // string label_filename = 4;
  void clear_label_filename();
  const std::string& label_filename() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_label_filename(ArgT0&& arg0, ArgT... args);
  std::string* mutable_label_filename();
  PROTOBUF_NODISCARD std::string* release_label_filename();
  void set_allocated_label_filename(std::string* label_filename);
  private:
  const std::string& _internal_label_filename() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_label_filename(const std::string& value);
  std::string* _internal_mutable_label_filename();
  public:

  // .nvidia.inferenceserver.DataType data_type = 2;
  void clear_data_type();
  ::nvidia::inferenceserver::DataType data_type() const;
  void set_data_type(::nvidia::inferenceserver::DataType value);
  private:
  ::nvidia::inferenceserver::DataType _internal_data_type() const;
  void _internal_set_data_type(::nvidia::inferenceserver::DataType value);
  public:

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOutput)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > dims_;
  mutable std::atomic<int> _dims_cached_byte_size_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr name_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr label_filename_;
  int data_type_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelVersionPolicy_Latest final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelVersionPolicy.Latest) */ {
 public:
  inline ModelVersionPolicy_Latest() : ModelVersionPolicy_Latest(nullptr) {}
  ~ModelVersionPolicy_Latest() override;
  explicit constexpr ModelVersionPolicy_Latest(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelVersionPolicy_Latest(const ModelVersionPolicy_Latest& from);
  ModelVersionPolicy_Latest(ModelVersionPolicy_Latest&& from) noexcept
    : ModelVersionPolicy_Latest() {
    *this = ::std::move(from);
  }

  inline ModelVersionPolicy_Latest& operator=(const ModelVersionPolicy_Latest& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelVersionPolicy_Latest& operator=(ModelVersionPolicy_Latest&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelVersionPolicy_Latest& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelVersionPolicy_Latest* internal_default_instance() {
    return reinterpret_cast<const ModelVersionPolicy_Latest*>(
               &_ModelVersionPolicy_Latest_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    3;

  friend void swap(ModelVersionPolicy_Latest& a, ModelVersionPolicy_Latest& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelVersionPolicy_Latest* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelVersionPolicy_Latest* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelVersionPolicy_Latest* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelVersionPolicy_Latest>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelVersionPolicy_Latest& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelVersionPolicy_Latest& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelVersionPolicy_Latest* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelVersionPolicy.Latest";
  }
  protected:
  explicit ModelVersionPolicy_Latest(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kNumVersionsFieldNumber = 1,
  };
  // uint32 num_versions = 1;
  void clear_num_versions();
  uint32_t num_versions() const;
  void set_num_versions(uint32_t value);
  private:
  uint32_t _internal_num_versions() const;
  void _internal_set_num_versions(uint32_t value);
  public:

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Latest)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  uint32_t num_versions_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelVersionPolicy_All final :
    public ::PROTOBUF_NAMESPACE_ID::internal::ZeroFieldsBase /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelVersionPolicy.All) */ {
 public:
  inline ModelVersionPolicy_All() : ModelVersionPolicy_All(nullptr) {}
  explicit constexpr ModelVersionPolicy_All(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelVersionPolicy_All(const ModelVersionPolicy_All& from);
  ModelVersionPolicy_All(ModelVersionPolicy_All&& from) noexcept
    : ModelVersionPolicy_All() {
    *this = ::std::move(from);
  }

  inline ModelVersionPolicy_All& operator=(const ModelVersionPolicy_All& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelVersionPolicy_All& operator=(ModelVersionPolicy_All&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelVersionPolicy_All& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelVersionPolicy_All* internal_default_instance() {
    return reinterpret_cast<const ModelVersionPolicy_All*>(
               &_ModelVersionPolicy_All_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    4;

  friend void swap(ModelVersionPolicy_All& a, ModelVersionPolicy_All& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelVersionPolicy_All* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelVersionPolicy_All* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelVersionPolicy_All* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelVersionPolicy_All>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::internal::ZeroFieldsBase::CopyFrom;
  inline void CopyFrom(const ModelVersionPolicy_All& from) {
    ::PROTOBUF_NAMESPACE_ID::internal::ZeroFieldsBase::CopyImpl(this, from);
  }
  using ::PROTOBUF_NAMESPACE_ID::internal::ZeroFieldsBase::MergeFrom;
  void MergeFrom(const ModelVersionPolicy_All& from) {
    ::PROTOBUF_NAMESPACE_ID::internal::ZeroFieldsBase::MergeImpl(this, from);
  }
  public:

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelVersionPolicy.All";
  }
  protected:
  explicit ModelVersionPolicy_All(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.All)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelVersionPolicy_Specific final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelVersionPolicy.Specific) */ {
 public:
  inline ModelVersionPolicy_Specific() : ModelVersionPolicy_Specific(nullptr) {}
  ~ModelVersionPolicy_Specific() override;
  explicit constexpr ModelVersionPolicy_Specific(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelVersionPolicy_Specific(const ModelVersionPolicy_Specific& from);
  ModelVersionPolicy_Specific(ModelVersionPolicy_Specific&& from) noexcept
    : ModelVersionPolicy_Specific() {
    *this = ::std::move(from);
  }

  inline ModelVersionPolicy_Specific& operator=(const ModelVersionPolicy_Specific& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelVersionPolicy_Specific& operator=(ModelVersionPolicy_Specific&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelVersionPolicy_Specific& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelVersionPolicy_Specific* internal_default_instance() {
    return reinterpret_cast<const ModelVersionPolicy_Specific*>(
               &_ModelVersionPolicy_Specific_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    5;

  friend void swap(ModelVersionPolicy_Specific& a, ModelVersionPolicy_Specific& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelVersionPolicy_Specific* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelVersionPolicy_Specific* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelVersionPolicy_Specific* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelVersionPolicy_Specific>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelVersionPolicy_Specific& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelVersionPolicy_Specific& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelVersionPolicy_Specific* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelVersionPolicy.Specific";
  }
  protected:
  explicit ModelVersionPolicy_Specific(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kVersionsFieldNumber = 1,
  };
  // repeated int64 versions = 1;
  int versions_size() const;
  private:
  int _internal_versions_size() const;
  public:
  void clear_versions();
  private:
  int64_t _internal_versions(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      _internal_versions() const;
  void _internal_add_versions(int64_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      _internal_mutable_versions();
  public:
  int64_t versions(int index) const;
  void set_versions(int index, int64_t value);
  void add_versions(int64_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
      versions() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
      mutable_versions();

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Specific)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t > versions_;
  mutable std::atomic<int> _versions_cached_byte_size_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelVersionPolicy final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelVersionPolicy) */ {
 public:
  inline ModelVersionPolicy() : ModelVersionPolicy(nullptr) {}
  ~ModelVersionPolicy() override;
  explicit constexpr ModelVersionPolicy(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelVersionPolicy(const ModelVersionPolicy& from);
  ModelVersionPolicy(ModelVersionPolicy&& from) noexcept
    : ModelVersionPolicy() {
    *this = ::std::move(from);
  }

  inline ModelVersionPolicy& operator=(const ModelVersionPolicy& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelVersionPolicy& operator=(ModelVersionPolicy&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelVersionPolicy& default_instance() {
    return *internal_default_instance();
  }
  enum PolicyChoiceCase {
    kLatest = 1,
    kAll = 2,
    kSpecific = 3,
    POLICY_CHOICE_NOT_SET = 0,
  };

  static inline const ModelVersionPolicy* internal_default_instance() {
    return reinterpret_cast<const ModelVersionPolicy*>(
               &_ModelVersionPolicy_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    6;

  friend void swap(ModelVersionPolicy& a, ModelVersionPolicy& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelVersionPolicy* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelVersionPolicy* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelVersionPolicy* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelVersionPolicy>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelVersionPolicy& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelVersionPolicy& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelVersionPolicy* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelVersionPolicy";
  }
  protected:
  explicit ModelVersionPolicy(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelVersionPolicy_Latest Latest;
  typedef ModelVersionPolicy_All All;
  typedef ModelVersionPolicy_Specific Specific;

  // accessors -------------------------------------------------------

  enum : int {
    kLatestFieldNumber = 1,
    kAllFieldNumber = 2,
    kSpecificFieldNumber = 3,
  };
  // .nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;
  bool has_latest() const;
  private:
  bool _internal_has_latest() const;
  public:
  void clear_latest();
  const ::nvidia::inferenceserver::ModelVersionPolicy_Latest& latest() const;
  PROTOBUF_NODISCARD ::nvidia::inferenceserver::ModelVersionPolicy_Latest* release_latest();
  ::nvidia::inferenceserver::ModelVersionPolicy_Latest* mutable_latest();
  void set_allocated_latest(::nvidia::inferenceserver::ModelVersionPolicy_Latest* latest);
  private:
  const ::nvidia::inferenceserver::ModelVersionPolicy_Latest& _internal_latest() const;
  ::nvidia::inferenceserver::ModelVersionPolicy_Latest* _internal_mutable_latest();
  public:
  void unsafe_arena_set_allocated_latest(
      ::nvidia::inferenceserver::ModelVersionPolicy_Latest* latest);
  ::nvidia::inferenceserver::ModelVersionPolicy_Latest* unsafe_arena_release_latest();

  // .nvidia.inferenceserver.ModelVersionPolicy.All all = 2;
  bool has_all() const;
  private:
  bool _internal_has_all() const;
  public:
  void clear_all();
  const ::nvidia::inferenceserver::ModelVersionPolicy_All& all() const;
  PROTOBUF_NODISCARD ::nvidia::inferenceserver::ModelVersionPolicy_All* release_all();
  ::nvidia::inferenceserver::ModelVersionPolicy_All* mutable_all();
  void set_allocated_all(::nvidia::inferenceserver::ModelVersionPolicy_All* all);
  private:
  const ::nvidia::inferenceserver::ModelVersionPolicy_All& _internal_all() const;
  ::nvidia::inferenceserver::ModelVersionPolicy_All* _internal_mutable_all();
  public:
  void unsafe_arena_set_allocated_all(
      ::nvidia::inferenceserver::ModelVersionPolicy_All* all);
  ::nvidia::inferenceserver::ModelVersionPolicy_All* unsafe_arena_release_all();

  // .nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;
  bool has_specific() const;
  private:
  bool _internal_has_specific() const;
  public:
  void clear_specific();
  const ::nvidia::inferenceserver::ModelVersionPolicy_Specific& specific() const;
  PROTOBUF_NODISCARD ::nvidia::inferenceserver::ModelVersionPolicy_Specific* release_specific();
  ::nvidia::inferenceserver::ModelVersionPolicy_Specific* mutable_specific();
  void set_allocated_specific(::nvidia::inferenceserver::ModelVersionPolicy_Specific* specific);
  private:
  const ::nvidia::inferenceserver::ModelVersionPolicy_Specific& _internal_specific() const;
  ::nvidia::inferenceserver::ModelVersionPolicy_Specific* _internal_mutable_specific();
  public:
  void unsafe_arena_set_allocated_specific(
      ::nvidia::inferenceserver::ModelVersionPolicy_Specific* specific);
  ::nvidia::inferenceserver::ModelVersionPolicy_Specific* unsafe_arena_release_specific();

  void clear_policy_choice();
  PolicyChoiceCase policy_choice_case() const;
  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy)
 private:
  class _Internal;
  void set_has_latest();
  void set_has_all();
  void set_has_specific();

  inline bool has_policy_choice() const;
  inline void clear_has_policy_choice();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  union PolicyChoiceUnion {
    constexpr PolicyChoiceUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::nvidia::inferenceserver::ModelVersionPolicy_Latest* latest_;
    ::nvidia::inferenceserver::ModelVersionPolicy_All* all_;
    ::nvidia::inferenceserver::ModelVersionPolicy_Specific* specific_;
  } policy_choice_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelOptimizationPolicy_Graph final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOptimizationPolicy.Graph) */ {
 public:
  inline ModelOptimizationPolicy_Graph() : ModelOptimizationPolicy_Graph(nullptr) {}
  ~ModelOptimizationPolicy_Graph() override;
  explicit constexpr ModelOptimizationPolicy_Graph(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelOptimizationPolicy_Graph(const ModelOptimizationPolicy_Graph& from);
  ModelOptimizationPolicy_Graph(ModelOptimizationPolicy_Graph&& from) noexcept
    : ModelOptimizationPolicy_Graph() {
    *this = ::std::move(from);
  }

  inline ModelOptimizationPolicy_Graph& operator=(const ModelOptimizationPolicy_Graph& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelOptimizationPolicy_Graph& operator=(ModelOptimizationPolicy_Graph&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelOptimizationPolicy_Graph& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelOptimizationPolicy_Graph* internal_default_instance() {
    return reinterpret_cast<const ModelOptimizationPolicy_Graph*>(
               &_ModelOptimizationPolicy_Graph_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    7;

  friend void swap(ModelOptimizationPolicy_Graph& a, ModelOptimizationPolicy_Graph& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelOptimizationPolicy_Graph* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelOptimizationPolicy_Graph* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelOptimizationPolicy_Graph* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelOptimizationPolicy_Graph>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelOptimizationPolicy_Graph& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelOptimizationPolicy_Graph& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOptimizationPolicy_Graph* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelOptimizationPolicy.Graph";
  }
  protected:
  explicit ModelOptimizationPolicy_Graph(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kLevelFieldNumber = 1,
  };
  // int32 level = 1;
  void clear_level();
  int32_t level() const;
  void set_level(int32_t value);
  private:
  int32_t _internal_level() const;
  void _internal_set_level(int32_t value);
  public:

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int32_t level_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelOptimizationPolicy final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelOptimizationPolicy) */ {
 public:
  inline ModelOptimizationPolicy() : ModelOptimizationPolicy(nullptr) {}
  ~ModelOptimizationPolicy() override;
  explicit constexpr ModelOptimizationPolicy(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelOptimizationPolicy(const ModelOptimizationPolicy& from);
  ModelOptimizationPolicy(ModelOptimizationPolicy&& from) noexcept
    : ModelOptimizationPolicy() {
    *this = ::std::move(from);
  }

  inline ModelOptimizationPolicy& operator=(const ModelOptimizationPolicy& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelOptimizationPolicy& operator=(ModelOptimizationPolicy&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelOptimizationPolicy& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelOptimizationPolicy* internal_default_instance() {
    return reinterpret_cast<const ModelOptimizationPolicy*>(
               &_ModelOptimizationPolicy_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    8;

  friend void swap(ModelOptimizationPolicy& a, ModelOptimizationPolicy& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelOptimizationPolicy* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelOptimizationPolicy* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelOptimizationPolicy* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelOptimizationPolicy>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelOptimizationPolicy& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelOptimizationPolicy& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelOptimizationPolicy* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelOptimizationPolicy";
  }
  protected:
  explicit ModelOptimizationPolicy(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ModelOptimizationPolicy_Graph Graph;

  typedef ModelOptimizationPolicy_ModelPriority ModelPriority;
  static constexpr ModelPriority PRIORITY_DEFAULT =
    ModelOptimizationPolicy_ModelPriority_PRIORITY_DEFAULT;
  static constexpr ModelPriority PRIORITY_MAX =
    ModelOptimizationPolicy_ModelPriority_PRIORITY_MAX;
  static constexpr ModelPriority PRIORITY_MIN =
    ModelOptimizationPolicy_ModelPriority_PRIORITY_MIN;
  static inline bool ModelPriority_IsValid(int value) {
    return ModelOptimizationPolicy_ModelPriority_IsValid(value);
  }
  static constexpr ModelPriority ModelPriority_MIN =
    ModelOptimizationPolicy_ModelPriority_ModelPriority_MIN;
  static constexpr ModelPriority ModelPriority_MAX =
    ModelOptimizationPolicy_ModelPriority_ModelPriority_MAX;
  static constexpr int ModelPriority_ARRAYSIZE =
    ModelOptimizationPolicy_ModelPriority_ModelPriority_ARRAYSIZE;
  static inline const ::PROTOBUF_NAMESPACE_ID::EnumDescriptor*
  ModelPriority_descriptor() {
    return ModelOptimizationPolicy_ModelPriority_descriptor();
  }
  template<typename T>
  static inline const std::string& ModelPriority_Name(T enum_t_value) {
    static_assert(::std::is_same<T, ModelPriority>::value ||
      ::std::is_integral<T>::value,
      "Incorrect type passed to function ModelPriority_Name.");
    return ModelOptimizationPolicy_ModelPriority_Name(enum_t_value);
  }
  static inline bool ModelPriority_Parse(::PROTOBUF_NAMESPACE_ID::ConstStringParam name,
      ModelPriority* value) {
    return ModelOptimizationPolicy_ModelPriority_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  enum : int {
    kGraphFieldNumber = 1,
    kPriorityFieldNumber = 2,
  };
  // .nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;
  bool has_graph() const;
  private:
  bool _internal_has_graph() const;
  public:
  void clear_graph();
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph& graph() const;
  PROTOBUF_NODISCARD ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* release_graph();
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* mutable_graph();
  void set_allocated_graph(::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* graph);
  private:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph& _internal_graph() const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* _internal_mutable_graph();
  public:
  void unsafe_arena_set_allocated_graph(
      ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* graph);
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* unsafe_arena_release_graph();

  // .nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;
  void clear_priority();
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority priority() const;
  void set_priority(::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority value);
  private:
  ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority _internal_priority() const;
  void _internal_set_priority(::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority value);
  public:

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* graph_;
  int priority_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelDynamicBatching final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelDynamicBatching) */ {
 public:
  inline ModelDynamicBatching() : ModelDynamicBatching(nullptr) {}
  ~ModelDynamicBatching() override;
  explicit constexpr ModelDynamicBatching(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelDynamicBatching(const ModelDynamicBatching& from);
  ModelDynamicBatching(ModelDynamicBatching&& from) noexcept
    : ModelDynamicBatching() {
    *this = ::std::move(from);
  }

  inline ModelDynamicBatching& operator=(const ModelDynamicBatching& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelDynamicBatching& operator=(ModelDynamicBatching&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelDynamicBatching& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelDynamicBatching* internal_default_instance() {
    return reinterpret_cast<const ModelDynamicBatching*>(
               &_ModelDynamicBatching_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    9;

  friend void swap(ModelDynamicBatching& a, ModelDynamicBatching& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelDynamicBatching* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelDynamicBatching* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelDynamicBatching* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelDynamicBatching>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelDynamicBatching& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelDynamicBatching& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelDynamicBatching* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelDynamicBatching";
  }
  protected:
  explicit ModelDynamicBatching(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kPreferredBatchSizeFieldNumber = 1,
    kMaxQueueDelayMicrosecondsFieldNumber = 2,
  };
  // repeated int32 preferred_batch_size = 1;
  int preferred_batch_size_size() const;
  private:
  int _internal_preferred_batch_size_size() const;
  public:
  void clear_preferred_batch_size();
  private:
  int32_t _internal_preferred_batch_size(int index) const;
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >&
      _internal_preferred_batch_size() const;
  void _internal_add_preferred_batch_size(int32_t value);
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >*
      _internal_mutable_preferred_batch_size();
  public:
  int32_t preferred_batch_size(int index) const;
  void set_preferred_batch_size(int index, int32_t value);
  void add_preferred_batch_size(int32_t value);
  const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >&
      preferred_batch_size() const;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >*
      mutable_preferred_batch_size();

  // int32 max_queue_delay_microseconds = 2;
  void clear_max_queue_delay_microseconds();
  int32_t max_queue_delay_microseconds() const;
  void set_max_queue_delay_microseconds(int32_t value);
  private:
  int32_t _internal_max_queue_delay_microseconds() const;
  void _internal_set_max_queue_delay_microseconds(int32_t value);
  public:

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelDynamicBatching)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t > preferred_batch_size_;
  mutable std::atomic<int> _preferred_batch_size_cached_byte_size_;
  int32_t max_queue_delay_microseconds_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelSequenceBatching final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelSequenceBatching) */ {
 public:
  inline ModelSequenceBatching() : ModelSequenceBatching(nullptr) {}
  ~ModelSequenceBatching() override;
  explicit constexpr ModelSequenceBatching(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelSequenceBatching(const ModelSequenceBatching& from);
  ModelSequenceBatching(ModelSequenceBatching&& from) noexcept
    : ModelSequenceBatching() {
    *this = ::std::move(from);
  }

  inline ModelSequenceBatching& operator=(const ModelSequenceBatching& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelSequenceBatching& operator=(ModelSequenceBatching&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelSequenceBatching& default_instance() {
    return *internal_default_instance();
  }
  static inline const ModelSequenceBatching* internal_default_instance() {
    return reinterpret_cast<const ModelSequenceBatching*>(
               &_ModelSequenceBatching_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    10;

  friend void swap(ModelSequenceBatching& a, ModelSequenceBatching& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelSequenceBatching* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelSequenceBatching* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelSequenceBatching* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelSequenceBatching>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelSequenceBatching& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelSequenceBatching& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelSequenceBatching* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelSequenceBatching";
  }
  protected:
  explicit ModelSequenceBatching(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kMaxQueueDelayMicrosecondsFieldNumber = 1,
  };
  // int32 max_queue_delay_microseconds = 1;
  void clear_max_queue_delay_microseconds();
  int32_t max_queue_delay_microseconds() const;
  void set_max_queue_delay_microseconds(int32_t value);
  private:
  int32_t _internal_max_queue_delay_microseconds() const;
  void _internal_set_max_queue_delay_microseconds(int32_t value);
  public:

  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  int32_t max_queue_delay_microseconds_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// -------------------------------------------------------------------

class ModelConfig_CcModelFilenamesEntry_DoNotUse : public ::PROTOBUF_NAMESPACE_ID::internal::MapEntry<ModelConfig_CcModelFilenamesEntry_DoNotUse, 
    std::string, std::string,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING> {
public:
  typedef ::PROTOBUF_NAMESPACE_ID::internal::MapEntry<ModelConfig_CcModelFilenamesEntry_DoNotUse, 
    std::string, std::string,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING> SuperType;
  ModelConfig_CcModelFilenamesEntry_DoNotUse();
  explicit constexpr ModelConfig_CcModelFilenamesEntry_DoNotUse(
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);
  explicit ModelConfig_CcModelFilenamesEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  void MergeFrom(const ModelConfig_CcModelFilenamesEntry_DoNotUse& other);
  static const ModelConfig_CcModelFilenamesEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelConfig_CcModelFilenamesEntry_DoNotUse*>(&_ModelConfig_CcModelFilenamesEntry_DoNotUse_default_instance_); }
  static bool ValidateKey(std::string* s) {
    return ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(s->data(), static_cast<int>(s->size()), ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::PARSE, "nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry.key");
 }
  static bool ValidateValue(std::string* s) {
    return ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(s->data(), static_cast<int>(s->size()), ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::PARSE, "nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry.value");
 }
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;
};

// -------------------------------------------------------------------

class ModelConfig_TagsEntry_DoNotUse : public ::PROTOBUF_NAMESPACE_ID::internal::MapEntry<ModelConfig_TagsEntry_DoNotUse, 
    std::string, std::string,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING> {
public:
  typedef ::PROTOBUF_NAMESPACE_ID::internal::MapEntry<ModelConfig_TagsEntry_DoNotUse, 
    std::string, std::string,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING> SuperType;
  ModelConfig_TagsEntry_DoNotUse();
  explicit constexpr ModelConfig_TagsEntry_DoNotUse(
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);
  explicit ModelConfig_TagsEntry_DoNotUse(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  void MergeFrom(const ModelConfig_TagsEntry_DoNotUse& other);
  static const ModelConfig_TagsEntry_DoNotUse* internal_default_instance() { return reinterpret_cast<const ModelConfig_TagsEntry_DoNotUse*>(&_ModelConfig_TagsEntry_DoNotUse_default_instance_); }
  static bool ValidateKey(std::string* s) {
    return ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(s->data(), static_cast<int>(s->size()), ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::PARSE, "nvidia.inferenceserver.ModelConfig.TagsEntry.key");
 }
  static bool ValidateValue(std::string* s) {
    return ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(s->data(), static_cast<int>(s->size()), ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::PARSE, "nvidia.inferenceserver.ModelConfig.TagsEntry.value");
 }
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;
};

// -------------------------------------------------------------------

class ModelConfig final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:nvidia.inferenceserver.ModelConfig) */ {
 public:
  inline ModelConfig() : ModelConfig(nullptr) {}
  ~ModelConfig() override;
  explicit constexpr ModelConfig(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ModelConfig(const ModelConfig& from);
  ModelConfig(ModelConfig&& from) noexcept
    : ModelConfig() {
    *this = ::std::move(from);
  }

  inline ModelConfig& operator=(const ModelConfig& from) {
    CopyFrom(from);
    return *this;
  }
  inline ModelConfig& operator=(ModelConfig&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ModelConfig& default_instance() {
    return *internal_default_instance();
  }
  enum SchedulingChoiceCase {
    kDynamicBatching = 11,
    kSequenceBatching = 13,
    SCHEDULING_CHOICE_NOT_SET = 0,
  };

  static inline const ModelConfig* internal_default_instance() {
    return reinterpret_cast<const ModelConfig*>(
               &_ModelConfig_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    13;

  friend void swap(ModelConfig& a, ModelConfig& b) {
    a.Swap(&b);
  }
  inline void Swap(ModelConfig* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ModelConfig* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ModelConfig* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ModelConfig>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ModelConfig& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom(const ModelConfig& from);
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message* to, const ::PROTOBUF_NAMESPACE_ID::Message& from);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ModelConfig* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "nvidia.inferenceserver.ModelConfig";
  }
  protected:
  explicit ModelConfig(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  private:
  static void ArenaDtor(void* object);
  inline void RegisterArenaDtor(::PROTOBUF_NAMESPACE_ID::Arena* arena);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------


  // accessors -------------------------------------------------------

  enum : int {
    kInputFieldNumber = 5,
    kOutputFieldNumber = 6,
    kInstanceGroupFieldNumber = 7,
    kCcModelFilenamesFieldNumber = 9,
    kTagsFieldNumber = 10,
    kNameFieldNumber = 1,
    kPlatformFieldNumber = 2,
    kDefaultModelFilenameFieldNumber = 8,
    kVersionPolicyFieldNumber = 3,
    kOptimizationFieldNumber = 12,
    kMaxBatchSizeFieldNumber = 4,
    kDynamicBatchingFieldNumber = 11,
    kSequenceBatchingFieldNumber = 13,
  };
  // repeated .nvidia.inferenceserver.ModelInput input = 5;
  int input_size() const;
  private:
  int _internal_input_size() const;
  public:
  void clear_input();
  ::nvidia::inferenceserver::ModelInput* mutable_input(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput >*
      mutable_input();
  private:
  const ::nvidia::inferenceserver::ModelInput& _internal_input(int index) const;
  ::nvidia::inferenceserver::ModelInput* _internal_add_input();
  public:
  const ::nvidia::inferenceserver::ModelInput& input(int index) const;
  ::nvidia::inferenceserver::ModelInput* add_input();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput >&
      input() const;

  // repeated .nvidia.inferenceserver.ModelOutput output = 6;
  int output_size() const;
  private:
  int _internal_output_size() const;
  public:
  void clear_output();
  ::nvidia::inferenceserver::ModelOutput* mutable_output(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput >*
      mutable_output();
  private:
  const ::nvidia::inferenceserver::ModelOutput& _internal_output(int index) const;
  ::nvidia::inferenceserver::ModelOutput* _internal_add_output();
  public:
  const ::nvidia::inferenceserver::ModelOutput& output(int index) const;
  ::nvidia::inferenceserver::ModelOutput* add_output();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput >&
      output() const;

  // repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;
  int instance_group_size() const;
  private:
  int _internal_instance_group_size() const;
  public:
  void clear_instance_group();
  ::nvidia::inferenceserver::ModelInstanceGroup* mutable_instance_group(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup >*
      mutable_instance_group();
  private:
  const ::nvidia::inferenceserver::ModelInstanceGroup& _internal_instance_group(int index) const;
  ::nvidia::inferenceserver::ModelInstanceGroup* _internal_add_instance_group();
  public:
  const ::nvidia::inferenceserver::ModelInstanceGroup& instance_group(int index) const;
  ::nvidia::inferenceserver::ModelInstanceGroup* add_instance_group();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup >&
      instance_group() const;

  // map<string, string> cc_model_filenames = 9;
  int cc_model_filenames_size() const;
  private:
  int _internal_cc_model_filenames_size() const;
  public:
  void clear_cc_model_filenames();
  private:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
      _internal_cc_model_filenames() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
      _internal_mutable_cc_model_filenames();
  public:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
      cc_model_filenames() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
      mutable_cc_model_filenames();

  // map<string, string> tags = 10;
  int tags_size() const;
  private:
  int _internal_tags_size() const;
  public:
  void clear_tags();
  private:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
      _internal_tags() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
      _internal_mutable_tags();
  public:
  const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
      tags() const;
  ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
      mutable_tags();

  // string name = 1;
  void clear_name();
  const std::string& name() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_name(ArgT0&& arg0, ArgT... args);
  std::string* mutable_name();
  PROTOBUF_NODISCARD std::string* release_name();
  void set_allocated_name(std::string* name);
  private:
  const std::string& _internal_name() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_name(const std::string& value);
  std::string* _internal_mutable_name();
  public:

  // string platform = 2;
  void clear_platform();
  const std::string& platform() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_platform(ArgT0&& arg0, ArgT... args);
  std::string* mutable_platform();
  PROTOBUF_NODISCARD std::string* release_platform();
  void set_allocated_platform(std::string* platform);
  private:
  const std::string& _internal_platform() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_platform(const std::string& value);
  std::string* _internal_mutable_platform();
  public:

  // string default_model_filename = 8;
  void clear_default_model_filename();
  const std::string& default_model_filename() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_default_model_filename(ArgT0&& arg0, ArgT... args);
  std::string* mutable_default_model_filename();
  PROTOBUF_NODISCARD std::string* release_default_model_filename();
  void set_allocated_default_model_filename(std::string* default_model_filename);
  private:
  const std::string& _internal_default_model_filename() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_default_model_filename(const std::string& value);
  std::string* _internal_mutable_default_model_filename();
  public:

  // .nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;
  bool has_version_policy() const;
  private:
  bool _internal_has_version_policy() const;
  public:
  void clear_version_policy();
  const ::nvidia::inferenceserver::ModelVersionPolicy& version_policy() const;
  PROTOBUF_NODISCARD ::nvidia::inferenceserver::ModelVersionPolicy* release_version_policy();
  ::nvidia::inferenceserver::ModelVersionPolicy* mutable_version_policy();
  void set_allocated_version_policy(::nvidia::inferenceserver::ModelVersionPolicy* version_policy);
  private:
  const ::nvidia::inferenceserver::ModelVersionPolicy& _internal_version_policy() const;
  ::nvidia::inferenceserver::ModelVersionPolicy* _internal_mutable_version_policy();
  public:
  void unsafe_arena_set_allocated_version_policy(
      ::nvidia::inferenceserver::ModelVersionPolicy* version_policy);
  ::nvidia::inferenceserver::ModelVersionPolicy* unsafe_arena_release_version_policy();

  // .nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;
  bool has_optimization() const;
  private:
  bool _internal_has_optimization() const;
  public:
  void clear_optimization();
  const ::nvidia::inferenceserver::ModelOptimizationPolicy& optimization() const;
  PROTOBUF_NODISCARD ::nvidia::inferenceserver::ModelOptimizationPolicy* release_optimization();
  ::nvidia::inferenceserver::ModelOptimizationPolicy* mutable_optimization();
  void set_allocated_optimization(::nvidia::inferenceserver::ModelOptimizationPolicy* optimization);
  private:
  const ::nvidia::inferenceserver::ModelOptimizationPolicy& _internal_optimization() const;
  ::nvidia::inferenceserver::ModelOptimizationPolicy* _internal_mutable_optimization();
  public:
  void unsafe_arena_set_allocated_optimization(
      ::nvidia::inferenceserver::ModelOptimizationPolicy* optimization);
  ::nvidia::inferenceserver::ModelOptimizationPolicy* unsafe_arena_release_optimization();

  // int32 max_batch_size = 4;
  void clear_max_batch_size();
  int32_t max_batch_size() const;
  void set_max_batch_size(int32_t value);
  private:
  int32_t _internal_max_batch_size() const;
  void _internal_set_max_batch_size(int32_t value);
  public:

  // .nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;
  bool has_dynamic_batching() const;
  private:
  bool _internal_has_dynamic_batching() const;
  public:
  void clear_dynamic_batching();
  const ::nvidia::inferenceserver::ModelDynamicBatching& dynamic_batching() const;
  PROTOBUF_NODISCARD ::nvidia::inferenceserver::ModelDynamicBatching* release_dynamic_batching();
  ::nvidia::inferenceserver::ModelDynamicBatching* mutable_dynamic_batching();
  void set_allocated_dynamic_batching(::nvidia::inferenceserver::ModelDynamicBatching* dynamic_batching);
  private:
  const ::nvidia::inferenceserver::ModelDynamicBatching& _internal_dynamic_batching() const;
  ::nvidia::inferenceserver::ModelDynamicBatching* _internal_mutable_dynamic_batching();
  public:
  void unsafe_arena_set_allocated_dynamic_batching(
      ::nvidia::inferenceserver::ModelDynamicBatching* dynamic_batching);
  ::nvidia::inferenceserver::ModelDynamicBatching* unsafe_arena_release_dynamic_batching();

  // .nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;
  bool has_sequence_batching() const;
  private:
  bool _internal_has_sequence_batching() const;
  public:
  void clear_sequence_batching();
  const ::nvidia::inferenceserver::ModelSequenceBatching& sequence_batching() const;
  PROTOBUF_NODISCARD ::nvidia::inferenceserver::ModelSequenceBatching* release_sequence_batching();
  ::nvidia::inferenceserver::ModelSequenceBatching* mutable_sequence_batching();
  void set_allocated_sequence_batching(::nvidia::inferenceserver::ModelSequenceBatching* sequence_batching);
  private:
  const ::nvidia::inferenceserver::ModelSequenceBatching& _internal_sequence_batching() const;
  ::nvidia::inferenceserver::ModelSequenceBatching* _internal_mutable_sequence_batching();
  public:
  void unsafe_arena_set_allocated_sequence_batching(
      ::nvidia::inferenceserver::ModelSequenceBatching* sequence_batching);
  ::nvidia::inferenceserver::ModelSequenceBatching* unsafe_arena_release_sequence_batching();

  void clear_scheduling_choice();
  SchedulingChoiceCase scheduling_choice_case() const;
  // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig)
 private:
  class _Internal;
  void set_has_dynamic_batching();
  void set_has_sequence_batching();

  inline bool has_scheduling_choice() const;
  inline void clear_has_scheduling_choice();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput > input_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput > output_;
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup > instance_group_;
  ::PROTOBUF_NAMESPACE_ID::internal::MapField<
      ModelConfig_CcModelFilenamesEntry_DoNotUse,
      std::string, std::string,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING> cc_model_filenames_;
  ::PROTOBUF_NAMESPACE_ID::internal::MapField<
      ModelConfig_TagsEntry_DoNotUse,
      std::string, std::string,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING,
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::TYPE_STRING> tags_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr name_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr platform_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr default_model_filename_;
  ::nvidia::inferenceserver::ModelVersionPolicy* version_policy_;
  ::nvidia::inferenceserver::ModelOptimizationPolicy* optimization_;
  int32_t max_batch_size_;
  union SchedulingChoiceUnion {
    constexpr SchedulingChoiceUnion() : _constinit_{} {}
      ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
    ::nvidia::inferenceserver::ModelDynamicBatching* dynamic_batching_;
    ::nvidia::inferenceserver::ModelSequenceBatching* sequence_batching_;
  } scheduling_choice_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  uint32_t _oneof_case_[1];

  friend struct ::TableStruct_model_5fconfig_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// ModelInstanceGroup

// string name = 1;
inline void ModelInstanceGroup::clear_name() {
  name_.ClearToEmpty();
}
inline const std::string& ModelInstanceGroup::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInstanceGroup.name)
  return _internal_name();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ModelInstanceGroup::set_name(ArgT0&& arg0, ArgT... args) {
 
 name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.name)
}
inline std::string* ModelInstanceGroup::mutable_name() {
  std::string* _s = _internal_mutable_name();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelInstanceGroup.name)
  return _s;
}
inline const std::string& ModelInstanceGroup::_internal_name() const {
  return name_.Get();
}
inline void ModelInstanceGroup::_internal_set_name(const std::string& value) {
  
  name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* ModelInstanceGroup::_internal_mutable_name() {
  
  return name_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* ModelInstanceGroup::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelInstanceGroup.name)
  return name_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void ModelInstanceGroup::set_allocated_name(std::string* name) {
  if (name != nullptr) {
    
  } else {
    
  }
  name_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), name,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (name_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    name_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelInstanceGroup.name)
}

// .nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;
inline void ModelInstanceGroup::clear_kind() {
  kind_ = 0;
}
inline ::nvidia::inferenceserver::ModelInstanceGroup_Kind ModelInstanceGroup::_internal_kind() const {
  return static_cast< ::nvidia::inferenceserver::ModelInstanceGroup_Kind >(kind_);
}
inline ::nvidia::inferenceserver::ModelInstanceGroup_Kind ModelInstanceGroup::kind() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInstanceGroup.kind)
  return _internal_kind();
}
inline void ModelInstanceGroup::_internal_set_kind(::nvidia::inferenceserver::ModelInstanceGroup_Kind value) {
  
  kind_ = value;
}
inline void ModelInstanceGroup::set_kind(::nvidia::inferenceserver::ModelInstanceGroup_Kind value) {
  _internal_set_kind(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.kind)
}

// int32 count = 2;
inline void ModelInstanceGroup::clear_count() {
  count_ = 0;
}
inline int32_t ModelInstanceGroup::_internal_count() const {
  return count_;
}
inline int32_t ModelInstanceGroup::count() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInstanceGroup.count)
  return _internal_count();
}
inline void ModelInstanceGroup::_internal_set_count(int32_t value) {
  
  count_ = value;
}
inline void ModelInstanceGroup::set_count(int32_t value) {
  _internal_set_count(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.count)
}

// repeated int32 gpus = 3;
inline int ModelInstanceGroup::_internal_gpus_size() const {
  return gpus_.size();
}
inline int ModelInstanceGroup::gpus_size() const {
  return _internal_gpus_size();
}
inline void ModelInstanceGroup::clear_gpus() {
  gpus_.Clear();
}
inline int32_t ModelInstanceGroup::_internal_gpus(int index) const {
  return gpus_.Get(index);
}
inline int32_t ModelInstanceGroup::gpus(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInstanceGroup.gpus)
  return _internal_gpus(index);
}
inline void ModelInstanceGroup::set_gpus(int index, int32_t value) {
  gpus_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInstanceGroup.gpus)
}
inline void ModelInstanceGroup::_internal_add_gpus(int32_t value) {
  gpus_.Add(value);
}
inline void ModelInstanceGroup::add_gpus(int32_t value) {
  _internal_add_gpus(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelInstanceGroup.gpus)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >&
ModelInstanceGroup::_internal_gpus() const {
  return gpus_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >&
ModelInstanceGroup::gpus() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelInstanceGroup.gpus)
  return _internal_gpus();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >*
ModelInstanceGroup::_internal_mutable_gpus() {
  return &gpus_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >*
ModelInstanceGroup::mutable_gpus() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelInstanceGroup.gpus)
  return _internal_mutable_gpus();
}

// -------------------------------------------------------------------

// ModelInput

// string name = 1;
inline void ModelInput::clear_name() {
  name_.ClearToEmpty();
}
inline const std::string& ModelInput::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.name)
  return _internal_name();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ModelInput::set_name(ArgT0&& arg0, ArgT... args) {
 
 name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.name)
}
inline std::string* ModelInput::mutable_name() {
  std::string* _s = _internal_mutable_name();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelInput.name)
  return _s;
}
inline const std::string& ModelInput::_internal_name() const {
  return name_.Get();
}
inline void ModelInput::_internal_set_name(const std::string& value) {
  
  name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* ModelInput::_internal_mutable_name() {
  
  return name_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* ModelInput::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelInput.name)
  return name_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void ModelInput::set_allocated_name(std::string* name) {
  if (name != nullptr) {
    
  } else {
    
  }
  name_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), name,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (name_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    name_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelInput.name)
}

// .nvidia.inferenceserver.DataType data_type = 2;
inline void ModelInput::clear_data_type() {
  data_type_ = 0;
}
inline ::nvidia::inferenceserver::DataType ModelInput::_internal_data_type() const {
  return static_cast< ::nvidia::inferenceserver::DataType >(data_type_);
}
inline ::nvidia::inferenceserver::DataType ModelInput::data_type() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.data_type)
  return _internal_data_type();
}
inline void ModelInput::_internal_set_data_type(::nvidia::inferenceserver::DataType value) {
  
  data_type_ = value;
}
inline void ModelInput::set_data_type(::nvidia::inferenceserver::DataType value) {
  _internal_set_data_type(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.data_type)
}

// .nvidia.inferenceserver.ModelInput.Format format = 3;
inline void ModelInput::clear_format() {
  format_ = 0;
}
inline ::nvidia::inferenceserver::ModelInput_Format ModelInput::_internal_format() const {
  return static_cast< ::nvidia::inferenceserver::ModelInput_Format >(format_);
}
inline ::nvidia::inferenceserver::ModelInput_Format ModelInput::format() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.format)
  return _internal_format();
}
inline void ModelInput::_internal_set_format(::nvidia::inferenceserver::ModelInput_Format value) {
  
  format_ = value;
}
inline void ModelInput::set_format(::nvidia::inferenceserver::ModelInput_Format value) {
  _internal_set_format(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.format)
}

// repeated int64 dims = 4;
inline int ModelInput::_internal_dims_size() const {
  return dims_.size();
}
inline int ModelInput::dims_size() const {
  return _internal_dims_size();
}
inline void ModelInput::clear_dims() {
  dims_.Clear();
}
inline int64_t ModelInput::_internal_dims(int index) const {
  return dims_.Get(index);
}
inline int64_t ModelInput::dims(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelInput.dims)
  return _internal_dims(index);
}
inline void ModelInput::set_dims(int index, int64_t value) {
  dims_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelInput.dims)
}
inline void ModelInput::_internal_add_dims(int64_t value) {
  dims_.Add(value);
}
inline void ModelInput::add_dims(int64_t value) {
  _internal_add_dims(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelInput.dims)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ModelInput::_internal_dims() const {
  return dims_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ModelInput::dims() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelInput.dims)
  return _internal_dims();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ModelInput::_internal_mutable_dims() {
  return &dims_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ModelInput::mutable_dims() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelInput.dims)
  return _internal_mutable_dims();
}

// -------------------------------------------------------------------

// ModelOutput

// string name = 1;
inline void ModelOutput::clear_name() {
  name_.ClearToEmpty();
}
inline const std::string& ModelOutput::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.name)
  return _internal_name();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ModelOutput::set_name(ArgT0&& arg0, ArgT... args) {
 
 name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOutput.name)
}
inline std::string* ModelOutput::mutable_name() {
  std::string* _s = _internal_mutable_name();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOutput.name)
  return _s;
}
inline const std::string& ModelOutput::_internal_name() const {
  return name_.Get();
}
inline void ModelOutput::_internal_set_name(const std::string& value) {
  
  name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* ModelOutput::_internal_mutable_name() {
  
  return name_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* ModelOutput::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOutput.name)
  return name_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void ModelOutput::set_allocated_name(std::string* name) {
  if (name != nullptr) {
    
  } else {
    
  }
  name_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), name,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (name_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    name_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOutput.name)
}

// .nvidia.inferenceserver.DataType data_type = 2;
inline void ModelOutput::clear_data_type() {
  data_type_ = 0;
}
inline ::nvidia::inferenceserver::DataType ModelOutput::_internal_data_type() const {
  return static_cast< ::nvidia::inferenceserver::DataType >(data_type_);
}
inline ::nvidia::inferenceserver::DataType ModelOutput::data_type() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.data_type)
  return _internal_data_type();
}
inline void ModelOutput::_internal_set_data_type(::nvidia::inferenceserver::DataType value) {
  
  data_type_ = value;
}
inline void ModelOutput::set_data_type(::nvidia::inferenceserver::DataType value) {
  _internal_set_data_type(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOutput.data_type)
}

// repeated int64 dims = 3;
inline int ModelOutput::_internal_dims_size() const {
  return dims_.size();
}
inline int ModelOutput::dims_size() const {
  return _internal_dims_size();
}
inline void ModelOutput::clear_dims() {
  dims_.Clear();
}
inline int64_t ModelOutput::_internal_dims(int index) const {
  return dims_.Get(index);
}
inline int64_t ModelOutput::dims(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.dims)
  return _internal_dims(index);
}
inline void ModelOutput::set_dims(int index, int64_t value) {
  dims_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOutput.dims)
}
inline void ModelOutput::_internal_add_dims(int64_t value) {
  dims_.Add(value);
}
inline void ModelOutput::add_dims(int64_t value) {
  _internal_add_dims(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelOutput.dims)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ModelOutput::_internal_dims() const {
  return dims_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ModelOutput::dims() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelOutput.dims)
  return _internal_dims();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ModelOutput::_internal_mutable_dims() {
  return &dims_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ModelOutput::mutable_dims() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelOutput.dims)
  return _internal_mutable_dims();
}

// string label_filename = 4;
inline void ModelOutput::clear_label_filename() {
  label_filename_.ClearToEmpty();
}
inline const std::string& ModelOutput::label_filename() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOutput.label_filename)
  return _internal_label_filename();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ModelOutput::set_label_filename(ArgT0&& arg0, ArgT... args) {
 
 label_filename_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOutput.label_filename)
}
inline std::string* ModelOutput::mutable_label_filename() {
  std::string* _s = _internal_mutable_label_filename();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOutput.label_filename)
  return _s;
}
inline const std::string& ModelOutput::_internal_label_filename() const {
  return label_filename_.Get();
}
inline void ModelOutput::_internal_set_label_filename(const std::string& value) {
  
  label_filename_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* ModelOutput::_internal_mutable_label_filename() {
  
  return label_filename_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* ModelOutput::release_label_filename() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOutput.label_filename)
  return label_filename_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void ModelOutput::set_allocated_label_filename(std::string* label_filename) {
  if (label_filename != nullptr) {
    
  } else {
    
  }
  label_filename_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), label_filename,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (label_filename_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    label_filename_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOutput.label_filename)
}

// -------------------------------------------------------------------

// ModelVersionPolicy_Latest

// uint32 num_versions = 1;
inline void ModelVersionPolicy_Latest::clear_num_versions() {
  num_versions_ = 0u;
}
inline uint32_t ModelVersionPolicy_Latest::_internal_num_versions() const {
  return num_versions_;
}
inline uint32_t ModelVersionPolicy_Latest::num_versions() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.Latest.num_versions)
  return _internal_num_versions();
}
inline void ModelVersionPolicy_Latest::_internal_set_num_versions(uint32_t value) {
  
  num_versions_ = value;
}
inline void ModelVersionPolicy_Latest::set_num_versions(uint32_t value) {
  _internal_set_num_versions(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelVersionPolicy.Latest.num_versions)
}

// -------------------------------------------------------------------

// ModelVersionPolicy_All

// -------------------------------------------------------------------

// ModelVersionPolicy_Specific

// repeated int64 versions = 1;
inline int ModelVersionPolicy_Specific::_internal_versions_size() const {
  return versions_.size();
}
inline int ModelVersionPolicy_Specific::versions_size() const {
  return _internal_versions_size();
}
inline void ModelVersionPolicy_Specific::clear_versions() {
  versions_.Clear();
}
inline int64_t ModelVersionPolicy_Specific::_internal_versions(int index) const {
  return versions_.Get(index);
}
inline int64_t ModelVersionPolicy_Specific::versions(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
  return _internal_versions(index);
}
inline void ModelVersionPolicy_Specific::set_versions(int index, int64_t value) {
  versions_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
}
inline void ModelVersionPolicy_Specific::_internal_add_versions(int64_t value) {
  versions_.Add(value);
}
inline void ModelVersionPolicy_Specific::add_versions(int64_t value) {
  _internal_add_versions(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ModelVersionPolicy_Specific::_internal_versions() const {
  return versions_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >&
ModelVersionPolicy_Specific::versions() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
  return _internal_versions();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ModelVersionPolicy_Specific::_internal_mutable_versions() {
  return &versions_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int64_t >*
ModelVersionPolicy_Specific::mutable_versions() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelVersionPolicy.Specific.versions)
  return _internal_mutable_versions();
}

// -------------------------------------------------------------------

// ModelVersionPolicy

// .nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;
inline bool ModelVersionPolicy::_internal_has_latest() const {
  return policy_choice_case() == kLatest;
}
inline bool ModelVersionPolicy::has_latest() const {
  return _internal_has_latest();
}
inline void ModelVersionPolicy::set_has_latest() {
  _oneof_case_[0] = kLatest;
}
inline void ModelVersionPolicy::clear_latest() {
  if (_internal_has_latest()) {
    if (GetArenaForAllocation() == nullptr) {
      delete policy_choice_.latest_;
    }
    clear_has_policy_choice();
  }
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Latest* ModelVersionPolicy::release_latest() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelVersionPolicy.latest)
  if (_internal_has_latest()) {
    clear_has_policy_choice();
      ::nvidia::inferenceserver::ModelVersionPolicy_Latest* temp = policy_choice_.latest_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    policy_choice_.latest_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_Latest& ModelVersionPolicy::_internal_latest() const {
  return _internal_has_latest()
      ? *policy_choice_.latest_
      : reinterpret_cast< ::nvidia::inferenceserver::ModelVersionPolicy_Latest&>(::nvidia::inferenceserver::_ModelVersionPolicy_Latest_default_instance_);
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_Latest& ModelVersionPolicy::latest() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.latest)
  return _internal_latest();
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Latest* ModelVersionPolicy::unsafe_arena_release_latest() {
  // @@protoc_insertion_point(field_unsafe_arena_release:nvidia.inferenceserver.ModelVersionPolicy.latest)
  if (_internal_has_latest()) {
    clear_has_policy_choice();
    ::nvidia::inferenceserver::ModelVersionPolicy_Latest* temp = policy_choice_.latest_;
    policy_choice_.latest_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ModelVersionPolicy::unsafe_arena_set_allocated_latest(::nvidia::inferenceserver::ModelVersionPolicy_Latest* latest) {
  clear_policy_choice();
  if (latest) {
    set_has_latest();
    policy_choice_.latest_ = latest;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:nvidia.inferenceserver.ModelVersionPolicy.latest)
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Latest* ModelVersionPolicy::_internal_mutable_latest() {
  if (!_internal_has_latest()) {
    clear_policy_choice();
    set_has_latest();
    policy_choice_.latest_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelVersionPolicy_Latest >(GetArenaForAllocation());
  }
  return policy_choice_.latest_;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Latest* ModelVersionPolicy::mutable_latest() {
  ::nvidia::inferenceserver::ModelVersionPolicy_Latest* _msg = _internal_mutable_latest();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelVersionPolicy.latest)
  return _msg;
}

// .nvidia.inferenceserver.ModelVersionPolicy.All all = 2;
inline bool ModelVersionPolicy::_internal_has_all() const {
  return policy_choice_case() == kAll;
}
inline bool ModelVersionPolicy::has_all() const {
  return _internal_has_all();
}
inline void ModelVersionPolicy::set_has_all() {
  _oneof_case_[0] = kAll;
}
inline void ModelVersionPolicy::clear_all() {
  if (_internal_has_all()) {
    if (GetArenaForAllocation() == nullptr) {
      delete policy_choice_.all_;
    }
    clear_has_policy_choice();
  }
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_All* ModelVersionPolicy::release_all() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelVersionPolicy.all)
  if (_internal_has_all()) {
    clear_has_policy_choice();
      ::nvidia::inferenceserver::ModelVersionPolicy_All* temp = policy_choice_.all_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    policy_choice_.all_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_All& ModelVersionPolicy::_internal_all() const {
  return _internal_has_all()
      ? *policy_choice_.all_
      : reinterpret_cast< ::nvidia::inferenceserver::ModelVersionPolicy_All&>(::nvidia::inferenceserver::_ModelVersionPolicy_All_default_instance_);
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_All& ModelVersionPolicy::all() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.all)
  return _internal_all();
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_All* ModelVersionPolicy::unsafe_arena_release_all() {
  // @@protoc_insertion_point(field_unsafe_arena_release:nvidia.inferenceserver.ModelVersionPolicy.all)
  if (_internal_has_all()) {
    clear_has_policy_choice();
    ::nvidia::inferenceserver::ModelVersionPolicy_All* temp = policy_choice_.all_;
    policy_choice_.all_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ModelVersionPolicy::unsafe_arena_set_allocated_all(::nvidia::inferenceserver::ModelVersionPolicy_All* all) {
  clear_policy_choice();
  if (all) {
    set_has_all();
    policy_choice_.all_ = all;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:nvidia.inferenceserver.ModelVersionPolicy.all)
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_All* ModelVersionPolicy::_internal_mutable_all() {
  if (!_internal_has_all()) {
    clear_policy_choice();
    set_has_all();
    policy_choice_.all_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelVersionPolicy_All >(GetArenaForAllocation());
  }
  return policy_choice_.all_;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_All* ModelVersionPolicy::mutable_all() {
  ::nvidia::inferenceserver::ModelVersionPolicy_All* _msg = _internal_mutable_all();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelVersionPolicy.all)
  return _msg;
}

// .nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;
inline bool ModelVersionPolicy::_internal_has_specific() const {
  return policy_choice_case() == kSpecific;
}
inline bool ModelVersionPolicy::has_specific() const {
  return _internal_has_specific();
}
inline void ModelVersionPolicy::set_has_specific() {
  _oneof_case_[0] = kSpecific;
}
inline void ModelVersionPolicy::clear_specific() {
  if (_internal_has_specific()) {
    if (GetArenaForAllocation() == nullptr) {
      delete policy_choice_.specific_;
    }
    clear_has_policy_choice();
  }
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Specific* ModelVersionPolicy::release_specific() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelVersionPolicy.specific)
  if (_internal_has_specific()) {
    clear_has_policy_choice();
      ::nvidia::inferenceserver::ModelVersionPolicy_Specific* temp = policy_choice_.specific_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    policy_choice_.specific_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_Specific& ModelVersionPolicy::_internal_specific() const {
  return _internal_has_specific()
      ? *policy_choice_.specific_
      : reinterpret_cast< ::nvidia::inferenceserver::ModelVersionPolicy_Specific&>(::nvidia::inferenceserver::_ModelVersionPolicy_Specific_default_instance_);
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy_Specific& ModelVersionPolicy::specific() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelVersionPolicy.specific)
  return _internal_specific();
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Specific* ModelVersionPolicy::unsafe_arena_release_specific() {
  // @@protoc_insertion_point(field_unsafe_arena_release:nvidia.inferenceserver.ModelVersionPolicy.specific)
  if (_internal_has_specific()) {
    clear_has_policy_choice();
    ::nvidia::inferenceserver::ModelVersionPolicy_Specific* temp = policy_choice_.specific_;
    policy_choice_.specific_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ModelVersionPolicy::unsafe_arena_set_allocated_specific(::nvidia::inferenceserver::ModelVersionPolicy_Specific* specific) {
  clear_policy_choice();
  if (specific) {
    set_has_specific();
    policy_choice_.specific_ = specific;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:nvidia.inferenceserver.ModelVersionPolicy.specific)
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Specific* ModelVersionPolicy::_internal_mutable_specific() {
  if (!_internal_has_specific()) {
    clear_policy_choice();
    set_has_specific();
    policy_choice_.specific_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelVersionPolicy_Specific >(GetArenaForAllocation());
  }
  return policy_choice_.specific_;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy_Specific* ModelVersionPolicy::mutable_specific() {
  ::nvidia::inferenceserver::ModelVersionPolicy_Specific* _msg = _internal_mutable_specific();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelVersionPolicy.specific)
  return _msg;
}

inline bool ModelVersionPolicy::has_policy_choice() const {
  return policy_choice_case() != POLICY_CHOICE_NOT_SET;
}
inline void ModelVersionPolicy::clear_has_policy_choice() {
  _oneof_case_[0] = POLICY_CHOICE_NOT_SET;
}
inline ModelVersionPolicy::PolicyChoiceCase ModelVersionPolicy::policy_choice_case() const {
  return ModelVersionPolicy::PolicyChoiceCase(_oneof_case_[0]);
}
// -------------------------------------------------------------------

// ModelOptimizationPolicy_Graph

// int32 level = 1;
inline void ModelOptimizationPolicy_Graph::clear_level() {
  level_ = 0;
}
inline int32_t ModelOptimizationPolicy_Graph::_internal_level() const {
  return level_;
}
inline int32_t ModelOptimizationPolicy_Graph::level() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.Graph.level)
  return _internal_level();
}
inline void ModelOptimizationPolicy_Graph::_internal_set_level(int32_t value) {
  
  level_ = value;
}
inline void ModelOptimizationPolicy_Graph::set_level(int32_t value) {
  _internal_set_level(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOptimizationPolicy.Graph.level)
}

// -------------------------------------------------------------------

// ModelOptimizationPolicy

// .nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;
inline bool ModelOptimizationPolicy::_internal_has_graph() const {
  return this != internal_default_instance() && graph_ != nullptr;
}
inline bool ModelOptimizationPolicy::has_graph() const {
  return _internal_has_graph();
}
inline void ModelOptimizationPolicy::clear_graph() {
  if (GetArenaForAllocation() == nullptr && graph_ != nullptr) {
    delete graph_;
  }
  graph_ = nullptr;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph& ModelOptimizationPolicy::_internal_graph() const {
  const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* p = graph_;
  return p != nullptr ? *p : reinterpret_cast<const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph&>(
      ::nvidia::inferenceserver::_ModelOptimizationPolicy_Graph_default_instance_);
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph& ModelOptimizationPolicy::graph() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.graph)
  return _internal_graph();
}
inline void ModelOptimizationPolicy::unsafe_arena_set_allocated_graph(
    ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* graph) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(graph_);
  }
  graph_ = graph;
  if (graph) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:nvidia.inferenceserver.ModelOptimizationPolicy.graph)
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* ModelOptimizationPolicy::release_graph() {
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* temp = graph_;
  graph_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* ModelOptimizationPolicy::unsafe_arena_release_graph() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelOptimizationPolicy.graph)
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* temp = graph_;
  graph_ = nullptr;
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* ModelOptimizationPolicy::_internal_mutable_graph() {
  
  if (graph_ == nullptr) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy_Graph>(GetArenaForAllocation());
    graph_ = p;
  }
  return graph_;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* ModelOptimizationPolicy::mutable_graph() {
  ::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* _msg = _internal_mutable_graph();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelOptimizationPolicy.graph)
  return _msg;
}
inline void ModelOptimizationPolicy::set_allocated_graph(::nvidia::inferenceserver::ModelOptimizationPolicy_Graph* graph) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete graph_;
  }
  if (graph) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::nvidia::inferenceserver::ModelOptimizationPolicy_Graph>::GetOwningArena(graph);
    if (message_arena != submessage_arena) {
      graph = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, graph, submessage_arena);
    }
    
  } else {
    
  }
  graph_ = graph;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelOptimizationPolicy.graph)
}

// .nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;
inline void ModelOptimizationPolicy::clear_priority() {
  priority_ = 0;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority ModelOptimizationPolicy::_internal_priority() const {
  return static_cast< ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority >(priority_);
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority ModelOptimizationPolicy::priority() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelOptimizationPolicy.priority)
  return _internal_priority();
}
inline void ModelOptimizationPolicy::_internal_set_priority(::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority value) {
  
  priority_ = value;
}
inline void ModelOptimizationPolicy::set_priority(::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority value) {
  _internal_set_priority(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelOptimizationPolicy.priority)
}

// -------------------------------------------------------------------

// ModelDynamicBatching

// repeated int32 preferred_batch_size = 1;
inline int ModelDynamicBatching::_internal_preferred_batch_size_size() const {
  return preferred_batch_size_.size();
}
inline int ModelDynamicBatching::preferred_batch_size_size() const {
  return _internal_preferred_batch_size_size();
}
inline void ModelDynamicBatching::clear_preferred_batch_size() {
  preferred_batch_size_.Clear();
}
inline int32_t ModelDynamicBatching::_internal_preferred_batch_size(int index) const {
  return preferred_batch_size_.Get(index);
}
inline int32_t ModelDynamicBatching::preferred_batch_size(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
  return _internal_preferred_batch_size(index);
}
inline void ModelDynamicBatching::set_preferred_batch_size(int index, int32_t value) {
  preferred_batch_size_.Set(index, value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
}
inline void ModelDynamicBatching::_internal_add_preferred_batch_size(int32_t value) {
  preferred_batch_size_.Add(value);
}
inline void ModelDynamicBatching::add_preferred_batch_size(int32_t value) {
  _internal_add_preferred_batch_size(value);
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >&
ModelDynamicBatching::_internal_preferred_batch_size() const {
  return preferred_batch_size_;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >&
ModelDynamicBatching::preferred_batch_size() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
  return _internal_preferred_batch_size();
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >*
ModelDynamicBatching::_internal_mutable_preferred_batch_size() {
  return &preferred_batch_size_;
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedField< int32_t >*
ModelDynamicBatching::mutable_preferred_batch_size() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size)
  return _internal_mutable_preferred_batch_size();
}

// int32 max_queue_delay_microseconds = 2;
inline void ModelDynamicBatching::clear_max_queue_delay_microseconds() {
  max_queue_delay_microseconds_ = 0;
}
inline int32_t ModelDynamicBatching::_internal_max_queue_delay_microseconds() const {
  return max_queue_delay_microseconds_;
}
inline int32_t ModelDynamicBatching::max_queue_delay_microseconds() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelDynamicBatching.max_queue_delay_microseconds)
  return _internal_max_queue_delay_microseconds();
}
inline void ModelDynamicBatching::_internal_set_max_queue_delay_microseconds(int32_t value) {
  
  max_queue_delay_microseconds_ = value;
}
inline void ModelDynamicBatching::set_max_queue_delay_microseconds(int32_t value) {
  _internal_set_max_queue_delay_microseconds(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelDynamicBatching.max_queue_delay_microseconds)
}

// -------------------------------------------------------------------

// ModelSequenceBatching

// int32 max_queue_delay_microseconds = 1;
inline void ModelSequenceBatching::clear_max_queue_delay_microseconds() {
  max_queue_delay_microseconds_ = 0;
}
inline int32_t ModelSequenceBatching::_internal_max_queue_delay_microseconds() const {
  return max_queue_delay_microseconds_;
}
inline int32_t ModelSequenceBatching::max_queue_delay_microseconds() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelSequenceBatching.max_queue_delay_microseconds)
  return _internal_max_queue_delay_microseconds();
}
inline void ModelSequenceBatching::_internal_set_max_queue_delay_microseconds(int32_t value) {
  
  max_queue_delay_microseconds_ = value;
}
inline void ModelSequenceBatching::set_max_queue_delay_microseconds(int32_t value) {
  _internal_set_max_queue_delay_microseconds(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelSequenceBatching.max_queue_delay_microseconds)
}

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// ModelConfig

// string name = 1;
inline void ModelConfig::clear_name() {
  name_.ClearToEmpty();
}
inline const std::string& ModelConfig::name() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.name)
  return _internal_name();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ModelConfig::set_name(ArgT0&& arg0, ArgT... args) {
 
 name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelConfig.name)
}
inline std::string* ModelConfig::mutable_name() {
  std::string* _s = _internal_mutable_name();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.name)
  return _s;
}
inline const std::string& ModelConfig::_internal_name() const {
  return name_.Get();
}
inline void ModelConfig::_internal_set_name(const std::string& value) {
  
  name_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* ModelConfig::_internal_mutable_name() {
  
  return name_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* ModelConfig::release_name() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.name)
  return name_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void ModelConfig::set_allocated_name(std::string* name) {
  if (name != nullptr) {
    
  } else {
    
  }
  name_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), name,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (name_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    name_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.name)
}

// string platform = 2;
inline void ModelConfig::clear_platform() {
  platform_.ClearToEmpty();
}
inline const std::string& ModelConfig::platform() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.platform)
  return _internal_platform();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ModelConfig::set_platform(ArgT0&& arg0, ArgT... args) {
 
 platform_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelConfig.platform)
}
inline std::string* ModelConfig::mutable_platform() {
  std::string* _s = _internal_mutable_platform();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.platform)
  return _s;
}
inline const std::string& ModelConfig::_internal_platform() const {
  return platform_.Get();
}
inline void ModelConfig::_internal_set_platform(const std::string& value) {
  
  platform_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* ModelConfig::_internal_mutable_platform() {
  
  return platform_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* ModelConfig::release_platform() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.platform)
  return platform_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void ModelConfig::set_allocated_platform(std::string* platform) {
  if (platform != nullptr) {
    
  } else {
    
  }
  platform_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), platform,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (platform_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    platform_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.platform)
}

// .nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;
inline bool ModelConfig::_internal_has_version_policy() const {
  return this != internal_default_instance() && version_policy_ != nullptr;
}
inline bool ModelConfig::has_version_policy() const {
  return _internal_has_version_policy();
}
inline void ModelConfig::clear_version_policy() {
  if (GetArenaForAllocation() == nullptr && version_policy_ != nullptr) {
    delete version_policy_;
  }
  version_policy_ = nullptr;
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy& ModelConfig::_internal_version_policy() const {
  const ::nvidia::inferenceserver::ModelVersionPolicy* p = version_policy_;
  return p != nullptr ? *p : reinterpret_cast<const ::nvidia::inferenceserver::ModelVersionPolicy&>(
      ::nvidia::inferenceserver::_ModelVersionPolicy_default_instance_);
}
inline const ::nvidia::inferenceserver::ModelVersionPolicy& ModelConfig::version_policy() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.version_policy)
  return _internal_version_policy();
}
inline void ModelConfig::unsafe_arena_set_allocated_version_policy(
    ::nvidia::inferenceserver::ModelVersionPolicy* version_policy) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(version_policy_);
  }
  version_policy_ = version_policy;
  if (version_policy) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:nvidia.inferenceserver.ModelConfig.version_policy)
}
inline ::nvidia::inferenceserver::ModelVersionPolicy* ModelConfig::release_version_policy() {
  
  ::nvidia::inferenceserver::ModelVersionPolicy* temp = version_policy_;
  version_policy_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy* ModelConfig::unsafe_arena_release_version_policy() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.version_policy)
  
  ::nvidia::inferenceserver::ModelVersionPolicy* temp = version_policy_;
  version_policy_ = nullptr;
  return temp;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy* ModelConfig::_internal_mutable_version_policy() {
  
  if (version_policy_ == nullptr) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelVersionPolicy>(GetArenaForAllocation());
    version_policy_ = p;
  }
  return version_policy_;
}
inline ::nvidia::inferenceserver::ModelVersionPolicy* ModelConfig::mutable_version_policy() {
  ::nvidia::inferenceserver::ModelVersionPolicy* _msg = _internal_mutable_version_policy();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.version_policy)
  return _msg;
}
inline void ModelConfig::set_allocated_version_policy(::nvidia::inferenceserver::ModelVersionPolicy* version_policy) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete version_policy_;
  }
  if (version_policy) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::nvidia::inferenceserver::ModelVersionPolicy>::GetOwningArena(version_policy);
    if (message_arena != submessage_arena) {
      version_policy = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, version_policy, submessage_arena);
    }
    
  } else {
    
  }
  version_policy_ = version_policy;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.version_policy)
}

// int32 max_batch_size = 4;
inline void ModelConfig::clear_max_batch_size() {
  max_batch_size_ = 0;
}
inline int32_t ModelConfig::_internal_max_batch_size() const {
  return max_batch_size_;
}
inline int32_t ModelConfig::max_batch_size() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.max_batch_size)
  return _internal_max_batch_size();
}
inline void ModelConfig::_internal_set_max_batch_size(int32_t value) {
  
  max_batch_size_ = value;
}
inline void ModelConfig::set_max_batch_size(int32_t value) {
  _internal_set_max_batch_size(value);
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelConfig.max_batch_size)
}

// repeated .nvidia.inferenceserver.ModelInput input = 5;
inline int ModelConfig::_internal_input_size() const {
  return input_.size();
}
inline int ModelConfig::input_size() const {
  return _internal_input_size();
}
inline void ModelConfig::clear_input() {
  input_.Clear();
}
inline ::nvidia::inferenceserver::ModelInput* ModelConfig::mutable_input(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.input)
  return input_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput >*
ModelConfig::mutable_input() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelConfig.input)
  return &input_;
}
inline const ::nvidia::inferenceserver::ModelInput& ModelConfig::_internal_input(int index) const {
  return input_.Get(index);
}
inline const ::nvidia::inferenceserver::ModelInput& ModelConfig::input(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.input)
  return _internal_input(index);
}
inline ::nvidia::inferenceserver::ModelInput* ModelConfig::_internal_add_input() {
  return input_.Add();
}
inline ::nvidia::inferenceserver::ModelInput* ModelConfig::add_input() {
  ::nvidia::inferenceserver::ModelInput* _add = _internal_add_input();
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelConfig.input)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInput >&
ModelConfig::input() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelConfig.input)
  return input_;
}

// repeated .nvidia.inferenceserver.ModelOutput output = 6;
inline int ModelConfig::_internal_output_size() const {
  return output_.size();
}
inline int ModelConfig::output_size() const {
  return _internal_output_size();
}
inline void ModelConfig::clear_output() {
  output_.Clear();
}
inline ::nvidia::inferenceserver::ModelOutput* ModelConfig::mutable_output(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.output)
  return output_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput >*
ModelConfig::mutable_output() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelConfig.output)
  return &output_;
}
inline const ::nvidia::inferenceserver::ModelOutput& ModelConfig::_internal_output(int index) const {
  return output_.Get(index);
}
inline const ::nvidia::inferenceserver::ModelOutput& ModelConfig::output(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.output)
  return _internal_output(index);
}
inline ::nvidia::inferenceserver::ModelOutput* ModelConfig::_internal_add_output() {
  return output_.Add();
}
inline ::nvidia::inferenceserver::ModelOutput* ModelConfig::add_output() {
  ::nvidia::inferenceserver::ModelOutput* _add = _internal_add_output();
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelConfig.output)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelOutput >&
ModelConfig::output() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelConfig.output)
  return output_;
}

// .nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;
inline bool ModelConfig::_internal_has_optimization() const {
  return this != internal_default_instance() && optimization_ != nullptr;
}
inline bool ModelConfig::has_optimization() const {
  return _internal_has_optimization();
}
inline void ModelConfig::clear_optimization() {
  if (GetArenaForAllocation() == nullptr && optimization_ != nullptr) {
    delete optimization_;
  }
  optimization_ = nullptr;
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy& ModelConfig::_internal_optimization() const {
  const ::nvidia::inferenceserver::ModelOptimizationPolicy* p = optimization_;
  return p != nullptr ? *p : reinterpret_cast<const ::nvidia::inferenceserver::ModelOptimizationPolicy&>(
      ::nvidia::inferenceserver::_ModelOptimizationPolicy_default_instance_);
}
inline const ::nvidia::inferenceserver::ModelOptimizationPolicy& ModelConfig::optimization() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.optimization)
  return _internal_optimization();
}
inline void ModelConfig::unsafe_arena_set_allocated_optimization(
    ::nvidia::inferenceserver::ModelOptimizationPolicy* optimization) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(optimization_);
  }
  optimization_ = optimization;
  if (optimization) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:nvidia.inferenceserver.ModelConfig.optimization)
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy* ModelConfig::release_optimization() {
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy* temp = optimization_;
  optimization_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy* ModelConfig::unsafe_arena_release_optimization() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.optimization)
  
  ::nvidia::inferenceserver::ModelOptimizationPolicy* temp = optimization_;
  optimization_ = nullptr;
  return temp;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy* ModelConfig::_internal_mutable_optimization() {
  
  if (optimization_ == nullptr) {
    auto* p = CreateMaybeMessage<::nvidia::inferenceserver::ModelOptimizationPolicy>(GetArenaForAllocation());
    optimization_ = p;
  }
  return optimization_;
}
inline ::nvidia::inferenceserver::ModelOptimizationPolicy* ModelConfig::mutable_optimization() {
  ::nvidia::inferenceserver::ModelOptimizationPolicy* _msg = _internal_mutable_optimization();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.optimization)
  return _msg;
}
inline void ModelConfig::set_allocated_optimization(::nvidia::inferenceserver::ModelOptimizationPolicy* optimization) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete optimization_;
  }
  if (optimization) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper<::nvidia::inferenceserver::ModelOptimizationPolicy>::GetOwningArena(optimization);
    if (message_arena != submessage_arena) {
      optimization = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, optimization, submessage_arena);
    }
    
  } else {
    
  }
  optimization_ = optimization;
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.optimization)
}

// .nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;
inline bool ModelConfig::_internal_has_dynamic_batching() const {
  return scheduling_choice_case() == kDynamicBatching;
}
inline bool ModelConfig::has_dynamic_batching() const {
  return _internal_has_dynamic_batching();
}
inline void ModelConfig::set_has_dynamic_batching() {
  _oneof_case_[0] = kDynamicBatching;
}
inline void ModelConfig::clear_dynamic_batching() {
  if (_internal_has_dynamic_batching()) {
    if (GetArenaForAllocation() == nullptr) {
      delete scheduling_choice_.dynamic_batching_;
    }
    clear_has_scheduling_choice();
  }
}
inline ::nvidia::inferenceserver::ModelDynamicBatching* ModelConfig::release_dynamic_batching() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.dynamic_batching)
  if (_internal_has_dynamic_batching()) {
    clear_has_scheduling_choice();
      ::nvidia::inferenceserver::ModelDynamicBatching* temp = scheduling_choice_.dynamic_batching_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    scheduling_choice_.dynamic_batching_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::nvidia::inferenceserver::ModelDynamicBatching& ModelConfig::_internal_dynamic_batching() const {
  return _internal_has_dynamic_batching()
      ? *scheduling_choice_.dynamic_batching_
      : reinterpret_cast< ::nvidia::inferenceserver::ModelDynamicBatching&>(::nvidia::inferenceserver::_ModelDynamicBatching_default_instance_);
}
inline const ::nvidia::inferenceserver::ModelDynamicBatching& ModelConfig::dynamic_batching() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.dynamic_batching)
  return _internal_dynamic_batching();
}
inline ::nvidia::inferenceserver::ModelDynamicBatching* ModelConfig::unsafe_arena_release_dynamic_batching() {
  // @@protoc_insertion_point(field_unsafe_arena_release:nvidia.inferenceserver.ModelConfig.dynamic_batching)
  if (_internal_has_dynamic_batching()) {
    clear_has_scheduling_choice();
    ::nvidia::inferenceserver::ModelDynamicBatching* temp = scheduling_choice_.dynamic_batching_;
    scheduling_choice_.dynamic_batching_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ModelConfig::unsafe_arena_set_allocated_dynamic_batching(::nvidia::inferenceserver::ModelDynamicBatching* dynamic_batching) {
  clear_scheduling_choice();
  if (dynamic_batching) {
    set_has_dynamic_batching();
    scheduling_choice_.dynamic_batching_ = dynamic_batching;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:nvidia.inferenceserver.ModelConfig.dynamic_batching)
}
inline ::nvidia::inferenceserver::ModelDynamicBatching* ModelConfig::_internal_mutable_dynamic_batching() {
  if (!_internal_has_dynamic_batching()) {
    clear_scheduling_choice();
    set_has_dynamic_batching();
    scheduling_choice_.dynamic_batching_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelDynamicBatching >(GetArenaForAllocation());
  }
  return scheduling_choice_.dynamic_batching_;
}
inline ::nvidia::inferenceserver::ModelDynamicBatching* ModelConfig::mutable_dynamic_batching() {
  ::nvidia::inferenceserver::ModelDynamicBatching* _msg = _internal_mutable_dynamic_batching();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.dynamic_batching)
  return _msg;
}

// .nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;
inline bool ModelConfig::_internal_has_sequence_batching() const {
  return scheduling_choice_case() == kSequenceBatching;
}
inline bool ModelConfig::has_sequence_batching() const {
  return _internal_has_sequence_batching();
}
inline void ModelConfig::set_has_sequence_batching() {
  _oneof_case_[0] = kSequenceBatching;
}
inline void ModelConfig::clear_sequence_batching() {
  if (_internal_has_sequence_batching()) {
    if (GetArenaForAllocation() == nullptr) {
      delete scheduling_choice_.sequence_batching_;
    }
    clear_has_scheduling_choice();
  }
}
inline ::nvidia::inferenceserver::ModelSequenceBatching* ModelConfig::release_sequence_batching() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.sequence_batching)
  if (_internal_has_sequence_batching()) {
    clear_has_scheduling_choice();
      ::nvidia::inferenceserver::ModelSequenceBatching* temp = scheduling_choice_.sequence_batching_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    scheduling_choice_.sequence_batching_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching& ModelConfig::_internal_sequence_batching() const {
  return _internal_has_sequence_batching()
      ? *scheduling_choice_.sequence_batching_
      : reinterpret_cast< ::nvidia::inferenceserver::ModelSequenceBatching&>(::nvidia::inferenceserver::_ModelSequenceBatching_default_instance_);
}
inline const ::nvidia::inferenceserver::ModelSequenceBatching& ModelConfig::sequence_batching() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.sequence_batching)
  return _internal_sequence_batching();
}
inline ::nvidia::inferenceserver::ModelSequenceBatching* ModelConfig::unsafe_arena_release_sequence_batching() {
  // @@protoc_insertion_point(field_unsafe_arena_release:nvidia.inferenceserver.ModelConfig.sequence_batching)
  if (_internal_has_sequence_batching()) {
    clear_has_scheduling_choice();
    ::nvidia::inferenceserver::ModelSequenceBatching* temp = scheduling_choice_.sequence_batching_;
    scheduling_choice_.sequence_batching_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ModelConfig::unsafe_arena_set_allocated_sequence_batching(::nvidia::inferenceserver::ModelSequenceBatching* sequence_batching) {
  clear_scheduling_choice();
  if (sequence_batching) {
    set_has_sequence_batching();
    scheduling_choice_.sequence_batching_ = sequence_batching;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:nvidia.inferenceserver.ModelConfig.sequence_batching)
}
inline ::nvidia::inferenceserver::ModelSequenceBatching* ModelConfig::_internal_mutable_sequence_batching() {
  if (!_internal_has_sequence_batching()) {
    clear_scheduling_choice();
    set_has_sequence_batching();
    scheduling_choice_.sequence_batching_ = CreateMaybeMessage< ::nvidia::inferenceserver::ModelSequenceBatching >(GetArenaForAllocation());
  }
  return scheduling_choice_.sequence_batching_;
}
inline ::nvidia::inferenceserver::ModelSequenceBatching* ModelConfig::mutable_sequence_batching() {
  ::nvidia::inferenceserver::ModelSequenceBatching* _msg = _internal_mutable_sequence_batching();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.sequence_batching)
  return _msg;
}

// repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;
inline int ModelConfig::_internal_instance_group_size() const {
  return instance_group_.size();
}
inline int ModelConfig::instance_group_size() const {
  return _internal_instance_group_size();
}
inline void ModelConfig::clear_instance_group() {
  instance_group_.Clear();
}
inline ::nvidia::inferenceserver::ModelInstanceGroup* ModelConfig::mutable_instance_group(int index) {
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.instance_group)
  return instance_group_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup >*
ModelConfig::mutable_instance_group() {
  // @@protoc_insertion_point(field_mutable_list:nvidia.inferenceserver.ModelConfig.instance_group)
  return &instance_group_;
}
inline const ::nvidia::inferenceserver::ModelInstanceGroup& ModelConfig::_internal_instance_group(int index) const {
  return instance_group_.Get(index);
}
inline const ::nvidia::inferenceserver::ModelInstanceGroup& ModelConfig::instance_group(int index) const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.instance_group)
  return _internal_instance_group(index);
}
inline ::nvidia::inferenceserver::ModelInstanceGroup* ModelConfig::_internal_add_instance_group() {
  return instance_group_.Add();
}
inline ::nvidia::inferenceserver::ModelInstanceGroup* ModelConfig::add_instance_group() {
  ::nvidia::inferenceserver::ModelInstanceGroup* _add = _internal_add_instance_group();
  // @@protoc_insertion_point(field_add:nvidia.inferenceserver.ModelConfig.instance_group)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::nvidia::inferenceserver::ModelInstanceGroup >&
ModelConfig::instance_group() const {
  // @@protoc_insertion_point(field_list:nvidia.inferenceserver.ModelConfig.instance_group)
  return instance_group_;
}

// string default_model_filename = 8;
inline void ModelConfig::clear_default_model_filename() {
  default_model_filename_.ClearToEmpty();
}
inline const std::string& ModelConfig::default_model_filename() const {
  // @@protoc_insertion_point(field_get:nvidia.inferenceserver.ModelConfig.default_model_filename)
  return _internal_default_model_filename();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ModelConfig::set_default_model_filename(ArgT0&& arg0, ArgT... args) {
 
 default_model_filename_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:nvidia.inferenceserver.ModelConfig.default_model_filename)
}
inline std::string* ModelConfig::mutable_default_model_filename() {
  std::string* _s = _internal_mutable_default_model_filename();
  // @@protoc_insertion_point(field_mutable:nvidia.inferenceserver.ModelConfig.default_model_filename)
  return _s;
}
inline const std::string& ModelConfig::_internal_default_model_filename() const {
  return default_model_filename_.Get();
}
inline void ModelConfig::_internal_set_default_model_filename(const std::string& value) {
  
  default_model_filename_.Set(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, value, GetArenaForAllocation());
}
inline std::string* ModelConfig::_internal_mutable_default_model_filename() {
  
  return default_model_filename_.Mutable(::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::EmptyDefault{}, GetArenaForAllocation());
}
inline std::string* ModelConfig::release_default_model_filename() {
  // @@protoc_insertion_point(field_release:nvidia.inferenceserver.ModelConfig.default_model_filename)
  return default_model_filename_.Release(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), GetArenaForAllocation());
}
inline void ModelConfig::set_allocated_default_model_filename(std::string* default_model_filename) {
  if (default_model_filename != nullptr) {
    
  } else {
    
  }
  default_model_filename_.SetAllocated(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), default_model_filename,
      GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (default_model_filename_.IsDefault(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited())) {
    default_model_filename_.Set(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), "", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:nvidia.inferenceserver.ModelConfig.default_model_filename)
}

// map<string, string> cc_model_filenames = 9;
inline int ModelConfig::_internal_cc_model_filenames_size() const {
  return cc_model_filenames_.size();
}
inline int ModelConfig::cc_model_filenames_size() const {
  return _internal_cc_model_filenames_size();
}
inline void ModelConfig::clear_cc_model_filenames() {
  cc_model_filenames_.Clear();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
ModelConfig::_internal_cc_model_filenames() const {
  return cc_model_filenames_.GetMap();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
ModelConfig::cc_model_filenames() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelConfig.cc_model_filenames)
  return _internal_cc_model_filenames();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
ModelConfig::_internal_mutable_cc_model_filenames() {
  return cc_model_filenames_.MutableMap();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
ModelConfig::mutable_cc_model_filenames() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelConfig.cc_model_filenames)
  return _internal_mutable_cc_model_filenames();
}

// map<string, string> tags = 10;
inline int ModelConfig::_internal_tags_size() const {
  return tags_.size();
}
inline int ModelConfig::tags_size() const {
  return _internal_tags_size();
}
inline void ModelConfig::clear_tags() {
  tags_.Clear();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
ModelConfig::_internal_tags() const {
  return tags_.GetMap();
}
inline const ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >&
ModelConfig::tags() const {
  // @@protoc_insertion_point(field_map:nvidia.inferenceserver.ModelConfig.tags)
  return _internal_tags();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
ModelConfig::_internal_mutable_tags() {
  return tags_.MutableMap();
}
inline ::PROTOBUF_NAMESPACE_ID::Map< std::string, std::string >*
ModelConfig::mutable_tags() {
  // @@protoc_insertion_point(field_mutable_map:nvidia.inferenceserver.ModelConfig.tags)
  return _internal_mutable_tags();
}

inline bool ModelConfig::has_scheduling_choice() const {
  return scheduling_choice_case() != SCHEDULING_CHOICE_NOT_SET;
}
inline void ModelConfig::clear_has_scheduling_choice() {
  _oneof_case_[0] = SCHEDULING_CHOICE_NOT_SET;
}
inline ModelConfig::SchedulingChoiceCase ModelConfig::scheduling_choice_case() const {
  return ModelConfig::SchedulingChoiceCase(_oneof_case_[0]);
}
#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace inferenceserver
}  // namespace nvidia

PROTOBUF_NAMESPACE_OPEN

template <> struct is_proto_enum< ::nvidia::inferenceserver::ModelInstanceGroup_Kind> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ModelInstanceGroup_Kind>() {
  return ::nvidia::inferenceserver::ModelInstanceGroup_Kind_descriptor();
}
template <> struct is_proto_enum< ::nvidia::inferenceserver::ModelInput_Format> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ModelInput_Format>() {
  return ::nvidia::inferenceserver::ModelInput_Format_descriptor();
}
template <> struct is_proto_enum< ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority>() {
  return ::nvidia::inferenceserver::ModelOptimizationPolicy_ModelPriority_descriptor();
}
template <> struct is_proto_enum< ::nvidia::inferenceserver::DataType> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::nvidia::inferenceserver::DataType>() {
  return ::nvidia::inferenceserver::DataType_descriptor();
}

PROTOBUF_NAMESPACE_CLOSE

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_model_5fconfig_2eproto
